자연어 처리의 모든 것<br>
2. 자연어 처리와 딥러닝<br><br>

1) Recurrent Neural Network<br><br>

RNN은 현재 타임스텝에 대해 이전 스텝까지의 정보를 기반으로 예측값을 산출하는 구조의 딥러닝 모델이다.<br>
h_t-1과 h_t를 결합해 계산한다. <br>
동일한 rnn모듈 A가 매 타임스텝마다 재귀적으로 호출되면서 A모듈의 출력이 다음 타임스텝의 입력으로 들어간다.<br><br>

RNN 계산 방법<br>
1. 변수들에 대하여, h_t = f(h_t-1,x_t)의 함수를 통해 매 타임스텝마다 hidden state를 다시 구해준다.<br>
2. 이 때, W와 입력값 (x_t, h_t-1)으로 tan_h를 곱해서 h_t를 구해준다.<br>
3. 구해진 h_t, x_t를 입력으로 y_t 값을 산출하게 된다.<br><br>

다양한 타입의 RNN 모델<br>
1. one to one<br>
: [키, 몸무게, 나이]와 같은 정보를 입력값으로 할 때, 이를 통해 저혈압/고혈압인지 분류하는 형태의 태스크<br>
2. one to many<br>
: '이미지 캡셔닝'과 같이 하나의 이미지를 입력값으로 주면 설명글을 생성하는 태스크<br>
3. many to one<br>
: 감성 분석과 같이 문장을 넣으면 긍/부정 중 하나의 레이블로 분류하는 태스크<br>
4. many to many<br>
: 기계 번역과 같이 입력값을 끝까지 다 읽은 후, 번역된 문장을 출력해주는 태스크<br>
4. many to many<br>
: 비디오 분류와 같이 영상의 프레임 레벨에서 예측하는 태스크, 혹은 각 단어의 품사에 대해 태깅하는 POS와 같은 태스크<br><br>

2) Character-level Language Model<br>
언어 모델이란 이전에 등장한 문자열을 기반으로 다음 단어를 예측하는 태스크를 말한다. 그중에서도 캐릭터 레벨 언어 모델(character-level Language Model)은 문자 단위로 다음에 올 문자를 예측하는 언어 모델이다.<br>
예를 들어, 그림과 같이 맨 처음에 "h"가 주어지면 "e"를 예측하고, "e"가 주어지면 "l"을 예측하고, "l"이 주어지면 다음 "l"을 예측하도록 hidden state가 학습돼야한다.<br>
각 타임스텝별로 output layer를 통해 차원이 4(유니크한 문자의 개수) 벡터를 출력해주는데 이를 logit이라고 부르며, softmax layer를 통과시키면 원-핫 벡터 형태의 출력값이 나오게 된다.<br><br>

3) Backpropagation through time and Long-Term-Dependency<br>
BPTT(Backpropagation through time)의 작동 방식<br>
BPTT란, Backpropagation through time의 줄임말로 RNN에서 타임스텝마다 계산된 weight를 backward propagation을 통해 학습하는 방식을 의미한다.<br>
BPTT를 통해 gradient를 계산해주면, tanh 로 감싸진 괄호안의 값들 중에 3 값이 속미분되어 나오게 된다. time step이 3이므로 3이 2번 곱해지지만, 만약 길이가 더 길어진다면 미분값은 기하급수적으로 커질 것이고, 만약 속미분되어 나오는 W의 값이 1보다 작다면 미분값은 기하급수적으로 작아질 것이다. 이 계산과정을 통해 Gradient Vanishing/Exploding 문제가 발생하고 이 문제가 Long-Term-Dependency를 일으키게 된다.<br><br>

4) Long Short-Term Memory (LSTM)<br>
STM의 중심 아이디어는 단기 기억으로 저장하여 이걸 때에 따라 꺼내 사용함으로 더 오래 기억할 수 있도록 개선하는 것<br>
Cell state에는 핵심 정보들을 모두 담아두고, 필요할 때마다 Hidden state를 가공해 time step에 필요한 정보만 노출하는 형태로 정보가 전파된다<br>
LSTM의 특징은 각 time step마다 필요한 정보를 단기 기억으로 hidden state에 저장하여 관리되도록 학습하는 것이다<br>
오차역전파(backpropagation) 진행시 가중치(W)를 계속해서 곱해주는 연산이 아니라, forget gate를 거친 값에 대해 필요로하는 정보를 덧셈을 통해 연산하여 그레디언트 소실/증폭 문제를 방지한다<br><br>

GRU : Gated Recurrent Unit<br>
Cell state, Hidden state를 일원화하여 경량화한 모델<br>
forget gate 대신 (1-input gate)를 사용하여  h_t를 구할때 가중평균의 형태로 계산한다<br>
계산량과 메모리 요구량을 LSTM에 비해 줄여준 모델이면서 동시에 성능면에서도 LSTM과 비슷하거나 더 좋은 성능을 내는 모델이다
