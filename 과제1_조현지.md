1. 딥러닝 소개

1) 신경망은 무엇인가?

딥러닝 : 신경망을 학습시키는 것

2) 신경망을 이용한 지도학습

목적에 따라 적절한 신경망을 사용하면 된다

neural network examples
•	standard nn
•	convolutional nn
•	recurrent nn

structured data
•	행과 열의 형태를 가진 데이터
•	잘 정의되어 있다

unstructured data
•	이미지, 음성, 텍스트,,,
•	구조가 정해져있지 않다

3) 왜 딥러닝이 뜨고 있을까요?

높은 성능을 내기 위한 방법
1.	충분히 큰 신경망
2.	많은 양의 데이터

따라서 
규모(신경망의 크기 = 많은 hidden unit, 많은 데이터)가 성능 향상에 중요하다

훈련할 데이터가 적다면 (small m)
구현방법에 따라 성능이 결정되는 경우가 많다

훈련 세트가 크다면 (big m)
large nn이 성능이 좋다

•	데이터
•	계산의 규모
•	알고리즘
이 신경망의 발전을 이루게 했다

idea -> code -> experiment 의 반복할때 빠른 계산은 딥러닝의 발전에 도움


2. 신경망과 로지스틱 회귀

1) 이진 분류

forward propagation -> backward propagation

로지스틱 회귀
: 이진 분류를 위한 알고리즘

64 * 64 사진
-> 빨간 초록 파란 각각 존재

feature vector -> 64 * 64 * 3

이진분류 목표:
입력된 사진을 나타내는 feature vector x를 가지고
y가 0인지 1인지 (고양이사진인지 아닌지) 를 예측한다

x : 훈련 샘플들
y : 출력될 레이블

훈련샘플의 갯수 : m
m_train : train 세트의 갯수. m_test : test 세트의 갯수

X : n_x * m 행렬
X.shape -> 행렬의 차원을 알 수 있다

Y : 1 * m 행렬

2) 로지스틱 회귀

:출력될 레이블 y가 0이나 1일 경우 (이진 분류일 경우) 쓰인다

y hat : 입력 특성 x가 주어졌을 때 y가 1일 확률
(사진이 고양이 사진일 확률)

x : nx 차원상의 벡터
w : nx 차원상의 벡터
b : 실수

y hat 구하는 방법은?
y hat은 0과 1 사이 값이어야 한다
-> 시그모이드 함수를 적용
-> z가 무한대일때 1에 가까워진다
-> z가 음수일수록 0에 가까워진다

—-또 다른 표기법
x0 = 1
x가 n_x+1 상의 차원의 벡터
y hat은 세타의 전치 * x의 시그모이드

세타0는 b

——b와 w를 분리하는 것이 편하기 때문에 쓰지 않을 것임


3) 로지스틱 회귀의 비용함수

매개변수들 w와 b를 학습하려면 비용함수를 정의해야 한다


x^(i) : i번째 훈련 샘플에 관한 데이터

L : loss funciton
y hat과 y 사이에 오차가 얼마나 큰지 측정

최적화를 위해 볼록한 함수를 이용한다

왜 이 함수를 쓰는가?
y가 1일 경우 손실값을 줄이려면 
: log(y hat)이 최대한 커져야 -> y hat이 최대한 커져야 (1에 수렴하길 원한다)
y가 0일 경우 손실값을 줄이려면
: log(1-y hat)이 최대한 커져야 -> y hat이 최대한 작아야(0에 수렴)

cost function
: 훈련 세트 전체에 대해 얼마나 잘 추출되었는지 측정
매개변수 w와 b가 train set를 잘 예측하는지 측정
(y의 예측값이 얼마나 좋은지를 각 훈련 샘플에 대한 참값과 비교해 측정)

loss function
: 하나의 훈련 샘플에 적용

로지스틱 회귀 모델을 학습하는 것
= cost function j를 최소화해주는 매개변수 w, b를 찾는 것

로지스틱 회귀는 작은 신경망과 같다


4) 경사하강법
: 매개변수 w와 b를 훈련 세트에 학습시키는 방법

cost function J(w,b)를 가장 작게 만드는 w와 b를 찾아야 한다

J(w,b)는 w, b 위의 곡면
-> 곡면의 높이는 J(w,b)의 값
-> J의 최솟값에 해당하는 w와 b를 찾아야 한다

cost function이 볼록하다
-> 지역 최적값이 하나

1.	w, b값을 한 값으로 초기화한다 (보통 0으로 설정)
2.	초기점에서 가장 가파른 내리막 방향으로 한 단계 내려간다
3.	반복하면 최적값에 도달한다

*경사 하강법
:= 값을 갱신한다는 의미

알파 = learning rate
경사 하강법을 반복할 때 한 단계의 크기를 결정한다

dw = 미분계수
왼쪽에서 초기화하던 오른쪽에서 초기화하던 전역 최솟값에 도달!

변수가 두개면 편미분 기호를 사용한다


5) 미분

도함수
f(a) = 3a
a=2 f(a)=6
a=2.001 f(a)=6.003

도함수 = 기울기 = height/width

함수의 어느곳이든 기울기가 3이다

6) 더 많은 미분 예제

함수의 다른 부분이 다른 기울기를 가지는 경우

7) 계산 그래프 (computation graph)

신경망의 계산
•	forward propagation : 신경망의 출력값을 계산
•	backward propagation : 경사나 도함수를 계산

J(a,b,c) = 3(a+bc)
1.	u = bc
2.	v = a + u
3.	J = 3v

특정한 출력값 변수를 최적화하고 싶을때 사용한다

왼쪽 -> 오른쪽 으로 계산

8) 계산 그래프로 미분하기

dJ/dv = ? (dv)

J = 3v
J의 증가량이 v의 증가량의 세배이기 때문에 3

back propagation

dJ/da = ? (da)

a를 바꾸면
-> v가 증가 (dv/da에 의한 양만큼)
-> J가 증가 

chain rule  -> dJ/da = dJ/dv * dv/da

final output = J
dJ/dv 코드에서 dvar 로 표시한다

—————-

dJ/du = dJ/dv * dv/du
dJ/db = dJ/du * du/db

9) 로지스틱 회귀의 경사하강법
(계산 그래프를 사용)

a : 로지스틱 회귀의 출력값
y : 참 값 레이블

로지스틱 회귀에서의 목적
: 매개변수 w와 b를 변경해서 손실을 줄이는 것

구하고자 하는 것 : 손실 함수의 도함수
연쇄법칙을 사용해 도함수를 구한다
1.	da (dL/da) 
2.	dz (dL/dz)
3.	dw1 (x1 * dz), dw2, db
4.	w1 := w1-알파dw1
      w2 := w2 - 알파dw2
      b := b- 알파db

10) m개 샘플의 경사하강법

J (cost function) : 각 손실의 평균
J의 도함수 : 각 손실 항 도함수의 평균

for i =1 to m
특성이 2개 (n=2)라고 가정

dw1, dw2, db는 값을 저장하는데 쓰고 있다
-> dw1은 w에 대한 전체 J의 도함수와 같다

2개의 for문
•	m개의 훈련 샘플 반복
•	특성을 반복 (n개)

명시적인 for문 -> 비효율적
따라서 for문 없애주는 vectorization 사용한다!!
