4. 최적화 알고리즘<br>
   <br>

1) 미니 배치 경사하강법<br>
   머신러닝이 잘 작동되는 모델을 찾기 위해선 많은 훈련을 반복적으로 거쳐야 한다<br>
   또 큰 데이터셋에서 훈련하는 것은 매우 느리다<br>
   따라서 좋은 최적화 알고리즘을 찾는 것이 중요하다<br>
   <br>
   벡터화 : m개의 샘플에 대한 계산을 반복문 없이 진행<br>
   X = [x^(1), x^(2), ..., x^(m)] X의 차원 : (n_x, m)<br>
   Y = [y^(1), y^(2), ..., y^(m)] Y의 차원 : (1, m)<br>
   <br>
   m이 5,000,000이거나 그보다 크다면?<br>
   <br>
   전체 훈련 세트에 대한 경사 하강법 구현하려면 <br>
   작은 한 단계를 밟기 전에 모든 훈련 세트를 처리해야 함<br>
   다음 단계를 밟기 전에 다시 오백만 개의 전체 훈련 샘플을 처리해야 함<br>
   <br>
   따라서 오백만 개의 거대한 훈련 샘플을 처리하기 전에 경사 하강법을 진행하면 더 빠른 알고리즘을 얻을 수 있다<br>
   처음 훈련 세트를 훈련하는 도중에라도 경사 하강의 단계를 진행시키기 위해 미니배치 경사 하강법 어떻게 사용하는지 알아보자!<br>
   <br>
   미니배치 : 나누어진 작은 훈련 세트<br>
   <br>
   ex) 미니배치가 1,000개인 경우<br>
   x^(1), x^(2), ..., x^(1000) -> 첫 번째 미니배치 X^{1}<br>
   x^(1001), x^(1002), ..., x^(2000) -> 두 번째 미니배치 X^{2}<br>
   총 5000,000개의 훈련 샘플 / 미니배치 1000개 -> 5000개의 미니배치 X^{5000}<br>
   <br>
   X^{t} Y^{t} : 1000개의 훈련 샘플<br>
   <br>
   x^(i) : i번째 훈련 샘플<br>
   z^[l] : l번째 신경망의 z값<br>
   t번째 미니배치 : X^{t} Y^{t} (n_x, 1000)차원, (1, 1000)차원<br>
   <br>

- 배치 경사 하강법 : 모든 훈련 세트를 동시에 진행시키는 방법<br>
  동시에 훈련 샘플의 모든 배치를 진행시킨다<br>
  <br>
- 미니배치 경사 하강법 : 전체 훈련 세트를 한 번에 진행시키지 않고 하나의 미니배치 X^{t} Y^{t}를 동시에 진행시킨다<br>
  <br>
  _미니배치 경사 하강법 실행 과정<br>
  t = 1, ..., 5000인 반복문 (1000인 미니배치 5000개)<br>
  : 한 단계의 경사 하강법을 구현<br>
  벡터화를 사용해 모든 1000개의 샘플을 동시에 진행시킨다<br> 1. X^{t}에 대해 정방향 전파 구현<br>
  z^[1] = W^[1]X <br>
  A^[1] = g^[1](Z^[1])<br>
  ...<br>
  A^[L] = g^[L](Z^[L])<br>
  벡터화된 구현이 오백만 개의 샘플 대신 1000개의 샘플을 진행한다<br> 2. 비용함수 J 계산<br>
  J^{t} = 1/1000_(i가 1부터 l까지 L(ŷ^(i), y^(i)) + λ/2*1000*(l에 대한 프로베니우스 norm의 제곱의 합)<br> 3. 역전파 <br>
  J^{t}에 대응하는 경사를 계산, 여전히 X^{t}, Y^{t} 사용<br> 4. 가중치 업데이트<br>
  W^[l] = W^[l] - αW^[l]<br>
  b^[l] = b^[l] - αb^[l]<br>
  <br>
  이것은 미니배치 경사 하강법을 사용한 훈련 세트를 지나는 한번 반복이다<br>
  (훈련의 한 epoch)<br>
  배치 경사 하강법 – 훈련 세트를 거치는 한 반복은 오직 하나의 경사 하강 단계만을 할 수 있게 한다<br>
  미니배치 경사 하강법 – 훈련 세트를 거치는 한 반복은 5,000개의 경사 하강 단계를 거치도록 한다, <br>
  다른 반복문을 사용해서 훈련 세트를 여러번 거치면 원하는 만큼 거의 수혐할 때까지 훈련 세트를 계속 반복시킨다.<br>
  훈련 세트가 많다면 미니배치 경사 하강법이 훨씬 더 빠르게 실행된다<br>
  <br>

2. 미니 배치 경사하강법 이해하기<br>

1) 배치 경사 하강법<br>
   : 모든 반복에서 전체 훈련 세트를 진행하고 각각의 반복마다 비용이 감소하기를 기대한다<br>
   <br>
   비용함수 J가 모든 반복마다 감소해야 한다<br>
   <br>
2) 미니배치 경사 하강법<br>
   : 모든 반복에서 어떤 X^{t}와 Y^{t}를 진행시킨다. 즉 모든 반복에서 다른 미니배치로 훈련한다 <br>
   <br>
   비용함수 J가 전체적인 흐름은 감소하지만 약간의 노이즈가 발생된다<br>
   노이즈가 발생하는 이유 : X^{1}와 Y^{1}이 X^{2}와 Y^{2} 보다 쉬운 미니배치라 비용이 낮을 때 (X^{2}와 Y^{2}에 잘못 표시된 샘플이 있다든지 등의 이유로 비용이 약간 더 높은 경우)<br>
   <br>
   ● 매개변수 고르기<br>
   훈련 세트의 크기 : m<br>
   미니 배치의 크기 : m<br>
   -> 배치 경사 하강법이 된다, 하나의 미니배치만을 갖게 된다 X^{1}과 Y^{1}<br>
   미니배치의 크기는 전체 훈련 세트와 같다<br>
   미니배치의 크기 : 1<br>
   -> 확률적 경사 하강법이 된다. 각각의 샘플은 하나의 미니배치이다. X^{1}과 Y^{1}은 첫 번째 훈련 샘플과 같다. 한 번에 하나의 훈련 샘플만을 사용한다.<br>
   <br>
   ● J의 등고선<br>
   배치 경사 하강법 : 상대적으로 노이즈가 적고 큰 스텝으로 최솟값으로 나아간다<br>
   확률적 경사 하강법 : 대부분의 경우 전역 최솟값으로 가지만 어떤 경우 잘못된 방향으로 간다. 따라서 노이즈가 많을 수 있지만 평균적으로는 좋은 방향으로 가게 된다. 따라서 진동하면서 최솟값의 주변을 돌아다니지만 절대 수렴하지 않는다<br>
   <br>
   ● 미니배치 크기를 1과 m사이로 설정하는 이유<br>
   배치 경사 하강법 – 미니 배치 크기 m <br>
   장점 : 매우 큰 훈련 세트를 모든 반복에서 진행<br>
   단점 : 한 반복에서 너무 오랜 시간이 걸린다<br>
   확률적 경사 하강법 – 미니 배치 크기 1<br>
   장점 : 하나의 샘플만 처리한 뒤에 계속 진행할 수 있다. 노이즈도 작은 학습률을 사용해 줄일 수 있다<br>
   단점 : 벡터화에서 얻을 수 있는 속도 향상을 잃게 된다. 한번에 하나의 훈련 세트를 진행하기 때문에 각 샘플을 진행하는 방식이 매우 비효율적이다.<br>
   따라서 미니배치 크기를 너무 크거나 작지 않게 설정해야 한다<br>
   장점 1 : 많은 벡터화를 얻는다<br>
   장점 2 : 전체 훈련 세트가 진행되기를 기다리지 않고 진행을 할 수 있다<br>
   <br>
   ● 1과 m사이 값을 어떻게 선택할까?<br>
3) 작은 훈련 세트(2000개보다 적은 경우)<br>
   : 배치 경사 하강법을 사용<br>
4) 미니배치 크기 64~512개(2^6~2^9)<br>
   : 2의 제곱수로 구현하기<br>
5) 미니배치에서 모든 X^{t}와 Y^{t}가 CPU와 GPU 메모리에 맞는지 확인<br>
   <br>

3. 지수 가중 이동 평균<br>
   경사 하강법보다 빠른 몇 가지 최적화 알고리즘을 이해하기 위해서는 지수가중평균을 사용할 수 있어야 한다<br>
   <br>
   런던 기온 데이터<br>

- 지역 평균이나 이동 평균의 흐름을 계산<br>
- 일별 기온의 지수가중평균<br>
  V*0 = 0<br>
  V_t = 0.9\*V*(t-1)(이전 날의 값) + 0.1*θ_t(해당 날의 기온)<br>
  0.9를 β로 바꾸면<br>
  V_t = β*V\_(t-1) + (1-β)*θ_t (대략적으로 1/(1-β)*일별 기온의 평균)v
- β = 0.9일 때 10일의 기온 평균<br>
- β = 0.5일 때 2일의 기온 평균<br>
  β값이 클수록 더 많은 날짜의 기온의 평균을 이용하기 때문에 선이 더 부드러워진다<br>
  하지만 더 큰 범위에서 기온을 평균하기 때문에 곡선이 올바른 값에서 멀어진다<br>
  <br>
  β값이 클수록 이전 값에 많은 가중치를 주고 현재의 기온에는 작은 가중치를 줘 더 느리게 적응한다, 선이 더 부드럽지만 지연되는 시간이 더 크다<br>
  <br>

4. 지수 가중 이동 평균 이해하기<br>
   V*t = β\*V*(t-1) + (1-β)*θ_t <br>
   <br>
   v_100 = 0.1*θ*100 + 0.9*v_99<br>
   v_99 = 0.1*θ_99 + 0.9*v_98<br>
   v_98 = 0.1*θ_98 + 0.9*v_97<br>
   <br>
   θ_100의 가중치의 평균<br>
   v100 = 0.1*θ_100 + 0.1*0.9*θ_99 + 0.1*(0.9)^2*θ_98​​ +⋯ <br>
   이를 그림으로 표현하면 지수 적으로 감소하는 그래프다. (v*​100을 기준으로 보았을 때), 그 이유는 v_​100은 각각의 요소에 지수적으로 감소하는 요소( 0.1×(0.9)^n )를 곱해서 더한 것이기 때문이다.<br>
   <br>
   얼마의 기간이 이동하면서 평균이 구해졌는가? 는 아래의 식으로 대략적으로 구할 수 있다<br>
   ● β=(1−ε) 라고 정의 하면<br>
   ● (1−ε)^​n = 1/e를 만족하는 n 이 그 기간이 되는데, 보통 1/εε으로 구할 수 있다.<br>
   지수 가중 이동 평균의 장점은 구현시 아주 적은 메모리를 사용한다는 것이다.<br>
   <br>
5. 지수 가중 이동 평균의 편향보정<br>
   편향 보정 -＞초기에 더 나은 추정값을 얻어 평균을 더 정확하게 계산<br>
   저번 시간에 따른 지수평균식대로라면 t=1 일때 (1−β) 를 곱한 값이 첫번째 값이 되는데, 이는 우리가 원하는 실제 v1값과 차이가 나게 된다.<br>
   따라서 vt/(1−β^t) 를 취해서 초기 값에서 실제값과 비슷해지게 한다. (t는 현재의 온도)<br>

- t가 더 커질수록 β^t는 0에 가까워진다<br>
- t가 충분히 커지면 편향 보정은 그 효과가 거의 없어진다<br>
- 초기 단계의 학습에서는 편향 보정은 더 나은 온도의 추정값을 얻을 수 있도록 도와준다<br>
  <br>

5. Momentum 최적화 알고리즘<br>
   ● Gradient descent example<br>
   : 최솟값으로 나아가면서 천천히 진동한다<br>
   위아래의 진동은 경사 하강법의 속도를 느리게 하고 더 큰 학습률을 사용하는 것을 막는다<br>

- 수직축 : 진동을 막기 위해 학습이 더 느리게 일어나기를 바람<br>
- 수평축 : 더 빠른 학습을 원함<br>
  <br>
  ● Momentum 알고리즘<br>
  : 반복 t에서 보편적인 도함수 dw와 db를 계산하게 된다<br>
- V_dW = β1\*V_dW + (1−β1)dW <br>
- w:=w−α\*V_dW<br>
  ​​경사의 평균을 구하면 수직 방향의 진동이 0에 가까운 값으로 평균이 만들어진다<br>
- 수직 방향에서는 양수와 음수를 평균하기 때문에 평균이 0이 된다<br>
- 수평 방향에서 모든 도함수는 오른쪽을 가리키고 있기 때문에 꽤 큰 값을 가진다<br>
  <br>
  결국 경사 하강법은 수직 방향에서는 훨씬 더 작은 진동이 있고, 수평 방향에서는 더 빠르게 움직인다<br>
  따라서 이 알고리즘은 더 직선의 길을 가거나 진동을 줄일 수 있게 한다!<br>
  ● 밥그릇 모양의 함수를 최소화 하려고 하면<br>
  ● 도함수의 항들 : 가속을 제공<br>
  ● 모멘텀 항들 : 속도를 나타냄<br>
  Momentum 의 장점은 매 단계의 경사 하강 정도를 부드럽게 만들어준다<br>
  Momentum 알고리즘에서는 보통 평향 추정을 실행하지 않는다. 이유는 step 이 10 단계정도 넘어가면 이동평균은 준비가 돼서 편향 추정이 더 이상 일어나지 않기 때문이다<br>
  <br>

6. RMSProp 최적화 알고리즘<br>
   수직 방향의 학습 속도를 낮추고 수평 방향의 속도를 빠르게 하기 위한 것<br>
   On iteration t :<br>
   compute dw, db on current mini-batch<br>
   s_dw = β*s_dw + (1-β)*dw^2 (요소별 제곱)<br>
   s_db = β*s_db + (1-β)*db^2<br>
   w := w - α*dw/√s_dw + ϵ<br>
   b := b - α*db/√s_db<br>
   <br>
   w방향(수평방향)에서는 학습률이 빠르길 원함<br>
   b방향(수직방향)에서는 진동을 줄이고 느리길 원함<br>
   ​<br>
   s_dw가 상대적으로 작고 s_db가 상대적으로 커서<br>
   수직 방향에서의 도함수 db >> 수평 방향에서의 도함수 dw<br>
   <br>
   RMSProp 의 장점은 미분값이 큰 곳에서는 업데이트 시 큰 값으로 나눠주기 때문에 기존 학습률 보다 작은 값으로 업데이트 된다. 따라서 진동을 줄이는데 도움이 된다. 반면 미분값이 작은 곳에서는 업데이트시 작은 값으로 나눠주기 때문에 기존 학습률보다 큰 값으로 업데이트 된다. 이는 더 빠르게 수렴하는 효과를 불러온다.<br>
   <br>
7. Adam 최적화 알고리즘<br>
   넓은 범위의 딥러닝 아키텍처에서 잘 작동하는 알고리즘<br>
   Momentum 과 RMSProp 을 섞은 알고리즘<br>

<br>
8) 학습률 감쇠<br>
알고리즘의 속도를 높이는 방법 : 시간에 따라 학습률을 천천히 줄이는 것<br>
<br>
● 왜 학습률 감쇠가 필요한가?<br>
작은 미니배치 일수록 잡음이 심해서 일정한 학습률이라면 최적값에 수렴하기 어려운 현상을 볼 수 있다.<br>
α가 큰 초기 단계에서는 상대적으로 빠른 학습이 가능하고 α가 작아지면 단계마다 진행 정도가 작아져 최솟값 주변의 밀집된 영역에서 진동한다.<br>
따라서 α를 천천히 줄여 학습 초기 단계에서는 큰 스텝으로 진행하고 학습이 수렴할수록 작은 스텝으로 진행하도록 한다.<br>
학습률 감쇠 기법을 사용하는 이유는 점점 학습률을 작게 줘서 최적값을 더 빨리 찾도록 만드는 것이다.<br>
<br>
● 학습률 감쇠를 구현하는 방법<br>
