4. 얕은 신경망 네트워크<br>

1) 신경망 네트워크 개요<br>
   신경망을 어떻게 구현하는가?<br>
   신경망은 시그모이드 유닛을 쌓아서 만들 수 있다<br>
   해당 노드는 z계산 -> a계산<br>
   다른 노드는 또다른 z계산 -> a계산<br>
   x^[i] -> I번 레이어<br>
   x^(i) -> I번째 훈련샘플<br>
   <br>
   로지스틱 회귀에선 z와a를 계산했지만<br>
   신경망에서는 z와 a를 여러 번 계산해준다<br>
   마지막으로 손실을 계산한다<br>
   <br>
   신경망에서도 마찬가지로 da^[2]와 dz^[2]를 계산하고 dW^[2]와 db^[2] 계산한다(역방향 계산)<br>
   <br>

2. 신경망 네트워크의 구성 알아보기<br>
   ▶ 은닉층이 하나인 신경망<br>
   신경망의 입력 층(input layer) a^[0] : x_1, x_2, x_3<br>
   신경망의 은닉 층(hidden layer) a^[1] -> 첫 번째 층<br>
   신경망의 출력 층(output layer) a^[2] -> 두 번째 층<br>
   예측 값 : ŷ<br>
   <br>
   지도 학습으로 훈련시키는 신경망에선 훈련 세트가 입력값 X와 출력값 Y로 이루어져 있다<br>
   은닉층의 실제 값은 훈련 세트에 기록되어 있지 않다 (은닉층의 값들은 알 수 없다)<br>
   <br>
   a : 활성값, 신경망의 층들이 다음 층으로 전달해주는 값<br>
   a^[0] : 입력층의 활성 값<br>
   a^[1] : 첫 번째 은닉층 -> 그림에서는 4차원 벡터(4개의 은닉 노드가 있기 때문)<br>
   a_1^[1] : 첫 번째 은닉층에 있는 첫 번째 유닛<br>
   a_1^[2] : 첫 번째 은닉층에 있는 두 번째 유닛<br>
   <br>
   신경망의 층을 셀 때 입력층은 세지 않는다<br>
   따라서 2 layer NN<br>
   <br>
   w : (4,3) 벡터<br>
   b : (4,1) 벡터<br>
   (왜냐하면 은닉 노드가 4개, 입력 특성이 3개이기 때문)<br>
   <br>
3. 신경망 네트워크 출력의 계산<br>
   ▶ 2층 신경망<br>
   ▶ 로지스틱 회귀<br>
   ▶ 신경망<br>
   로지스틱 회귀 계산을 반복<br>
   ai[l]<br>
   l : 몇 번째 층<br>
   i : 해당 층에서 몇 번째 노드<br>
   <br>
4. 많은 샘플에 대한 벡터화<br>
   a^​[i][j]<br>
   i : 몇 번째 층인지<br>
   j : 몇 번째 훈련 샘플인지<br>
   행렬Z와 A<br>
   가로 : 훈련 샘플의 번호<br>
   세로 : 신경망의 노드 (은닉 유닛)<br>
   <br>
   행렬 첫 번째행 첫 번째열 바로 아래값<br>
   = 첫 훈련샘플의 두 번째 은닉유닛의 활성값<br>
   <br>
   세로로 움직이면 훈련 샘플은 고정되고 은닉 유닛이 바뀐다<br>
   가로로 움직이면 은닉 유닛은 고정되고 훈련 샘플이 바뀐다<br>
   <br>
5. 벡터화 구현에 대한 설명<br>
   Z^[1](1) = W^[1]x^(1)+b^[1] - 훈련 샘플 1<br>
   Z^[1](2) = W^[1]x^(2)+b^[1] - 훈련 샘플 2<br>
   Z^[1](3) = W^[1]x^(3)+b^[1] - 훈련 샘플 3<br>
   <br>
   b를 0이라고 가정<br>
   W^[1]와 x^(1)의 곱은 열 벡터가 된다<br>
   행렬 X는 x^(1), x^(2), x^(3)를 모두 가로로 쌓아 만든 것<br>
   <br>
   W^[1]x^(i) = Z^[1](i)가 있었을 때 훈련 샘플을 다른 열에 채운다면<br>
   나오는 결과인 Z도 z가 열로 쌓인다<br>
   <br>
6. 활성화 함수<br>
   Sigmoid – 0부터 1까지 나타낸다<br>
   Tanh – 시그모이드와 비슷하지만 원점을 지나고 비율이 다르다<br>
   단점 : z가 굉장히 크거나 작으면 함수의 도함수가 굉장히 작아진다<br>
   <br>
   ReLU – z가 양수이면 도함수가 1이고 z가 음수이면 도함수가 0이다<br>
   leakyReLU – z가 음수일 때 도함수가 0인 대신 약간의 기울기를 준다 7)왜 비선형 활성화 함수를 써야하나?<br>
   비선형 활성화 함수 : ReLU, Sigmoid, Tanh 등의 함수<br>
   g(z)=z 라는 선형 활성화 함수를 사용한다고 가정했을 때, 3개의 은닉층을 쌓아도 g(g(g(z)))=z 로 아무런 혜택을 얻지 못했습니다. 따라서 은닉층에는 비선형 활성화 함수를 사용해야 한다<br>
   <br>
7. 신경망 네트워크와 경사 하강법<br>
   역전파 알고리즘에서 여섯 개의 식 유도<br>
   <br>
   axis = 1<br>
   keemdims = True<br>
   np.sum -> 어떤 축 방향으로 더할 때 사용<br>
   <br>
   단일층이 아닐 때는 1뿐만 아니라 1, 2, …,m 까지의 계산을 반복하면 된다<br>
   <br> 9)역전파에 대한 이해<br>
   da = -y/a + (1-y)/(1-a)<br>
   dz = a-y<br>
   dw = dzx<br>
   x는 고정값이기에 dx를 계산하지 않는다<br>
   <br> 10)랜덤 초기화<br>
   <br>
   신경망에서 w 의 초기값을 0으로 설정한 후 경사 하강법을 적용할 경우 올바르게 작동하지 않는다. dw 를 계산했을 때 모든 층이 같은 값을 가지게 되기 때문이다.<br>
   따라서 np.random.rand()를 이용해 0이 아닌 랜덤한 값을 부여해줘야 한다.<br>
