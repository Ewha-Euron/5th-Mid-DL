4. 얕은 신경망 네트워크

1) 신경망 네트워크 개요
   신경망을 어떻게 구현하는가?
   신경망은 시그모이드 유닛을 쌓아서 만들 수 있다
   해당 노드는 z계산 -> a계산
   다른 노드는 또다른 z계산 -> a계산
   x^[i] -> I번 레이어
   x^(i) -> I번째 훈련샘플

로지스틱 회귀에선 z와a를 계산했지만
신경망에서는 z와 a를 여러 번 계산해준다
마지막으로 손실을 계산한다

신경망에서도 마찬가지로 da^[2]와 dz^[2]를 계산하고 dW^[2]와 db^[2] 계산한다(역방향 계산)

2. 신경망 네트워크의 구성 알아보기
   ▶ 은닉층이 하나인 신경망
   신경망의 입력 층(input layer) a^[0] : x_1, x_2, x_3
   신경망의 은닉 층(hidden layer) a^[1] -> 첫 번째 층
   신경망의 출력 층(output layer) a^[2] -> 두 번째 층
   예측 값 : ŷ

지도 학습으로 훈련시키는 신경망에선 훈련 세트가 입력값 X와 출력값 Y로 이루어져 있다
은닉층의 실제 값은 훈련 세트에 기록되어 있지 않다 (은닉층의 값들은 알 수 없다)

a : 활성값, 신경망의 층들이 다음 층으로 전달해주는 값
a^[0] : 입력층의 활성 값
a^[1] : 첫 번째 은닉층 -> 그림에서는 4차원 벡터(4개의 은닉 노드가 있기 때문)
a_1^[1] : 첫 번째 은닉층에 있는 첫 번째 유닛
a_1^[2] : 첫 번째 은닉층에 있는 두 번째 유닛

신경망의 층을 셀 때 입력층은 세지 않는다
따라서 2 layer NN

w : (4,3) 벡터
b : (4,1) 벡터
(왜냐하면 은닉 노드가 4개, 입력 특성이 3개이기 때문)

3. 신경망 네트워크 출력의 계산
   ▶ 2층 신경망
   ▶ 로지스틱 회귀
   ▶ 신경망
   로지스틱 회귀 계산을 반복
   ai[l]
   l : 몇 번째 층
   i : 해당 층에서 몇 번째 노드

4. 많은 샘플에 대한 벡터화
   a^​[i][j]
   i : 몇 번째 층인지
   j : 몇 번째 훈련 샘플인지
   행렬Z와 A
   가로 : 훈련 샘플의 번호
   세로 : 신경망의 노드 (은닉 유닛)

행렬 첫 번째행 첫 번째열 바로 아래값
= 첫 훈련샘플의 두 번째 은닉유닛의 활성값

세로로 움직이면 훈련 샘플은 고정되고 은닉 유닛이 바뀐다
가로로 움직이면 은닉 유닛은 고정되고 훈련 샘플이 바뀐다

5. 벡터화 구현에 대한 설명
   Z^[1](1) = W^[1]x^(1)+b^[1] - 훈련 샘플 1
   Z^[1](2) = W^[1]x^(2)+b^[1] - 훈련 샘플 2
   Z^[1](3) = W^[1]x^(3)+b^[1] - 훈련 샘플 3

b를 0이라고 가정
W^[1]와 x^(1)의 곱은 열 벡터가 된다
행렬 X는 x^(1), x^(2), x^(3)를 모두 가로로 쌓아 만든 것

W^[1]x^(i) = Z^[1](i)가 있었을 때 훈련 샘플을 다른 열에 채운다면
나오는 결과인 Z도 z가 열로 쌓인다

6. 활성화 함수
   Sigmoid – 0부터 1까지 나타낸다
   Tanh – 시그모이드와 비슷하지만 원점을 지나고 비율이 다르다
   단점 : z가 굉장히 크거나 작으면 함수의 도함수가 굉장히 작아진다

ReLU – z가 양수이면 도함수가 1이고 z가 음수이면 도함수가 0이다
leakyReLU – z가 음수일 때 도함수가 0인 대신 약간의 기울기를 준다 7)왜 비선형 활성화 함수를 써야하나?
비선형 활성화 함수 : ReLU, Sigmoid, Tanh 등의 함수
g(z)=z 라는 선형 활성화 함수를 사용한다고 가정했을 때, 3개의 은닉층을 쌓아도 g(g(g(z)))=z 로 아무런 혜택을 얻지 못했습니다. 따라서 은닉층에는 비선형 활성화 함수를 사용해야 한다

8. 신경망 네트워크와 경사 하강법
   역전파 알고리즘에서 여섯 개의 식 유도

axis = 1
keemdims = True
np.sum -> 어떤 축 방향으로 더할 때 사용

단일층이 아닐 때는 1뿐만 아니라 1, 2, …,m 까지의 계산을 반복하면 된다

9)역전파에 대한 이해
da = -y/a + (1-y)/(1-a)
dz = a-y
dw = dzx
x는 고정값이기에 dx를 계산하지 않는다

10)랜덤 초기화

신경망에서 w 의 초기값을 0으로 설정한 후 경사 하강법을 적용할 경우 올바르게 작동하지 않는다. dw 를 계산했을 때 모든 층이 같은 값을 가지게 되기 때문이다.
따라서 np.random.rand()를 이용해 0이 아닌 랜덤한 값을 부여해줘야 한다.
