# 강의 요약

## 1. 딥러닝 소개

### (1). 환영합니다!

목차 소개

1. Neural Networks and Deep Learning → build cat recognizer
2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization
3. Structuring your Machine Learning project 
4. Convolutional Neural Networks (CNN) → often apply to images / how to built that
5. Natural Language Processing: Building sequence models → RNN, LSTM

---

### (2) 신경망은 무엇인가?

- Housing Price Prediction을 이용한 예시
    - size(x) → something(neuron) → price(y)
    - neuron은 주택의 크기를 입력으로 받아 price를 예측하는 역할을 한다.
    - 더 큰 신경망은 더 많은 뉴런으로 이루어져있다. (size뿐만 아니라 zip code, wealth, family size 등 다양한 요소를 고려하여 price를 예측하는 경우)

![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled.png)

- 위의 그림에서 4개의 x 입력을 받는다. input layer와 neuron layer는 면밀한 관계를 갖는다. 데이터가 더 주어질 수록 정확도는 올라간다.

---

### (3) 신경망을 이용한 지도학습

- 신경망의 성과는 대부분 지도학습에 의해 이루어진다. ex) 온라인 광고 추천 알고리즘
    - image : computer vision을 통해 이미지를 넣었을 때 tagging이 가능해짐
    - audio : Audio 파일로부터 text transcript 생성
    - translate :  English → Chinese
    - image, radar info
    
    → 분야에 따라 적용되는 신경망이 다르다
    
    - Neural Net examples
    
    ![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%201.png)
    
- Supervised Learning
    - Structured Data : Table로 정의되는 data
    - Unstructured Data : image, text, audio … / Deep learning 덕분에 비구조적 데이터에 대한 처리 효능이 매우 높아짐

---

### (4) 왜 딥러닝이 뜨고 있을까요?

- 데이터 양 증가, 컴퓨터 성능 향상, 알고리즘의 개선으로 인해 ( Data, Computation, Algorithms)
    - ex) ReLU함수를 이용한 gradient 소멸 문제 해결 (경사하강이 더 급격하게 이루어짐) → 계산 능력의 향상
    
    ![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%202.png)
    
- 더 큰 데이터, 더 큰 신경망을 이용해서 크게 성장할 수 있었다. (그러나 한계는 존재)
    - 어떻게 훈련시키느냐에 적은 데이터에서는 작은 신경망이 더 좋은 성능을 보일 수도 있다. 그러나 데이터가 많아지면 필연적으로 거대한 신경망이 더 좋은 성능을 보인다.

![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%203.png)

- 빠른 계산이 중요 → 아이디어를 코드로 구현한 후 다양한 실험을 시간 내에 진행할 수 있게됨. feedback 과정이 원활해짐

## 2. 신경망과 로지스틱 회귀

### (1) 이진 분류

- **Binary Classification**
    - ex : cat recognizer (1:cat vs 0:not cat)
    - colored img는 3개의 channel로 분류(RGB values를 각각 나타내는) 각 특징을 내포하는 각 채널(2D)에서 feature vector(x)(1D)로 정의
        
        64x64 크기의 RGB 채널이 있었다면 feature vetor= 64x64x3(n=12288)
        
        feature vector를 가지고 img에 대해 분류를 진행
        
    - **(x,y) x: n_x dimensional feature vector, y: label**
    - m=(m_train,m_test) : training example: (x^(1), y^(1)),(x^(2), y^(2)),(x^(3), y^(3))…,(x^(m), y^(m))  → x^(m)만 모아서 만든 행렬이 X로 정의
        
        **X : m개의 example에 대한 feature_vector들을 모아놓은 행렬**
        
        **Y : 1xm의 행렬로, y^(m)을 나열한 행렬**
        

---

### (2) 로지스틱 회귀 (Logistic Regression)

- 이진 분류 문제에 주로 쓰인다.
- parameters : **w**, **b** (real number) → pred_y를 구하는 방법 / y에 대한 예측을 위해 w와 b를 학습한다.
    
    output : pred_y= wx + b (y는 0~1사이어야 함) 
    → sigmoid func를 이용
    
    ⇒ **pred_y = sigmoid(wx+b)**
    

![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%204.png)

![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%205.png)

---

### (3) 로지스틱 회귀의 비용함수

- pred_y를 실제 y(label)에 가깝도록 w,b에 대한 학습을 위해 비용함수의 개념이 필요함
- **하나의 sample**에 대해 실제 y값과 비교하여 얼마나 잘 예측되었는지를 측정하는 지표

![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%206.png)

- **Loss Function(general)** :
    
    ![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%207.png)
    
    → logistic regression에선 최적화 함수가 non-convex 위와 같은 loss function을 사용하지 않는다. → global minimum을 찾기 어려워짐
    
    - **logistic regression의 loss function**
    
    ![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%208.png)
    
    → 1 case : y=1 → loss function = -log(y_pred) “y_pred은 1에 가까워야한다”
    
    → 2 case : y=0 → loss function = -log(1-y_pred) “y_pred은 0에 가까워야한다”
    
    (이 때 y_pred는 sigmoid에 의해 0~1 사이의 값)
    
- Cost Function(J(w,b)) : 손실 함수를 각각의 훈련 샘플에 적용한 값을 전체 샘플 수로 나눈 값, **전체 샘플에 대한 값**

![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%209.png)

---

### (4) 로지스틱 회귀의 비용함수

- Gradient Descent
    - J(w,b)를 최소(global optima)로하는 w,b를 찾아야한다. w는 차원의 제한이 없다.
        
        ![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%2010.png)
        
        일반적으로 초기값 w,b=0으로 설정
        
        ![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%2011.png)
        
        (α는 학습률을 의미), w의 위치에 따라 dw의 부호가 결정된다.
        

---

### (5) 미분 - (pass)

### (6) 더 많은 미분 예제 - (pass)

---

### (7) 계산 그래프

- J(a,b,c) = 3(a+bc)을 계산 그래프로 나타내기
    
    → 특정 출력값 변수를 최적화할 때 유용하다.
    
    1-step : u=bc
    
    2-step : v=a+u
    
    3-step : J=3v
    
    ![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%2012.png)
    

---

### (8) 계산 그래프로 미분하기

- 합성함수로 표현한 뒤 미분의 연쇄법칙을 이용해서 특정 변수에 대한 미분값을 구할 수 있다.
    
    위의 계산 그래프에선 j에 대한 변화량을 구하기 위해
    j←v←a의 단계를 거쳐 dj/da값을 구할 수 있다.
    
    이를 **역방향 전개**라고한다.
    
- 최종 변수를 Final output var, 미분하려고 하는 변수를 var라고 정의한다면 아래와 같이 정의된다.
    
    ![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%2013.png)
    

---

### (9) 로지스틱 회귀의 경사하강법

- **한 개의 sample**일 때 아래와 같이 정의할 수 있다.
    
    ![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%2014.png)
    
- 두 개의 feature x1, x2가 있다고 정의하면
    
    x1,w1,x2,w2,b 5개의 parameter가 생긴다.
    
    → z= w1x1 + w2x2 + b로 정의
    
    ![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%2015.png)
    
    역방향 전개를 통해 손실함수 L이 최소가 될 수 있는 a값을 구한다. (find dL/da)
    

---

### (10) m개 샘플의 경사하강법

- Logistic regression on m examples에서의 비용 함수는 아래와 같다.

![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%208b4f932df45748eabc48c921a175e89c/Untitled%2016.png)

→ 이 때 i가 1~m까지 진행될 때 a^(1), a^(2), a^(3)…a^(m)까지 각각의 a에 대한 dj/da를 계산한다.