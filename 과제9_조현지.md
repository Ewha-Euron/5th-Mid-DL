3. 최적화 문제 설정<br>

1) 입력값의 정규화<br>
   신경망의 훈련을 빠르게 할 수 있는 하나의 기법 : 입력을 정규화 하는 것<br>
   <br>
   두 개의 입력 특성이 있는 훈련 세트<br>
   입력 특성 x가 2차원<br>
   <br>
   입력을 정규화 하는 과정(2단계)<br>
   <br>

1. 평균을 0으로 만들기<br>
   0의 평균을 갖게 될 때까지 훈련 세트를 이동한다<br>
   <br>
2. 분산을 정규화하기 (1로 만들기)<br>
   특성 x1이 특성 x2보다 더 큰 분산을 갖고 있다<br>

<br>
그림으로 나타내면<br>

x1과 x2의 분산은 모두 1과 같다<br>

팁 : 이것을 훈련 데이터를 확대하는데 사용한다면 테스트 세트를 정규화할 때도 같은 μ와 σ를 사용해라 (훈련 세트와 테스트 세트를 같게 정규화하기 위해서)<br>

왜 입력 특성을 정규화하기를 원할까?<br>
비용함수의 정의 : <br>

정규화되지 않은 입력 특성을 사용했을 때 비용함수 모습은 매우 구부러진 활처럼 가늘고 긴 모양이다<br>
특성 x1이 1에서 1000, 특성 x2가 0에서 1의 값의 범위처럼 특성들이 매우 다른 크기를 갖고 있다면 매개변수에 대한 비율, 값의 범위는 w1과 w2가 굉장히 다른 값을 갖게 된다<br>
반면에 특성을 정규화하면 비용함수는 평균적으로 대칭적인 모양을 갖게 된다<br>
<br>
왼쪽 함수의 비용함수에 경사 하강법을 실행한다면 매우 작은 학습률을 사용하게 된다<br>
여기서 경사 하강법은 최종적으로 최솟값에 이르는 길을 찾기 전까지 앞뒤로 왔다 갔다하기 위해 많은 단계가 필요하다<br>
반면에 원 모양의 등고선의 경우 어디서 시작하든 경사 하강법은 최솟값으로 바로 갈 수 있다<br>
왔다갔다 하지 않아도 큰 스텝으로 전진할 수 있다<br>
실전에서는 w가 높은 차원인 벡터이고 2차원에 그리는 것이 모든 직관을 올바르게 전달하지 않는다 그러나 특성이 비슷한 크기를 갖을 때 비용함수가 더 둥글고 최적화하기 쉬운 모습이 된다는 대략적인 직관을 얻을 수 있다<br>
서로 비슷한 분산으로 비용함수 J를 최적화하기 쉽고 빠르게 만든다<br>
특성 x1이 0에서 1의 범위이고 x2가 –1에서 1, x3가 1에서 2인 경우에 (상당히 비슷한 범위를 갖을 때) 잘 작동한다<br>
범위가 너무 다르면 최적화 알고리즘에 방해가 된다<br>
평균을 0으로 설정하고 모든 특성이 비슷한 크기가 되도록 분산을 설정하면 학습 알고리즘이 빠르게 실행되는 것을 도울 수 있다!<br>
특성이 비슷한 크기를 갖는다면 이 과정을 중요하지 않다<br>
<br> 2) 경사소실/경사폭발<br>
매우 깊은 신경망을 훈련시키는 것의 문제 : 경사소실/경사폭발<br>
미분값 혹은 기울기가 아주 작아지거나 커질 수 있다<br>
*매우 깊은 신경망을 훈련시키는 경우<br>
<br>
매개변수 w^[1], w^[2], w^[3],...,w^[L] 갖는다<br>
활성화 함수 g(z)가 선형 활성화 함수를 사용한다고 가정<br>
b^[i]은 0이라고 가정<br>
출력 y= w^[L]*w^[L-1]_w^[L-2]_...*w^[3]*w^[2]*w^[1]*x<br>
w^[1]*x = z^[1] (b^[1]=0)<br>
a^[1] = g(z^[1]) = z^[1] (선형의 활성화 함수를 사용하기 때문)<br>
같은 이유로<br>
a^[2] = g(z^[2]) = g(w^[2]*a^[1])<br>
w^[2]*w^[1]*x = a^[2]<br>

<br>
w^[l] 가정<br>
<br>
각각의 행렬이 w^[l]과 같다고 가정하면 1.5 * 단위행렬이 된다<br>
y = 1.5^[L-1] * x<br>
L의 값이 크면 y의 예측값도 매우 커진다<br>
매우 깊은 신경망을 갖으면 y의 값은 폭발한다<br>
반대로 1보다 작은 값으로 교체하면 0.5^[L]<br>
y = 0.5^[L-1] * x<br>
<br>
층의 개수가 L인 함수의 경우<br>
x1, x2가 각각 1인 경우에 활성화는 1/2, 1/2, 1/4, 1/4, 1/8, 1/8,..., 1/2^L이 된다<br>
따라서 매우 깊은 네트워크의 경우 활성값은 기하급수적으로 감소한다<br>
<결론><br>
가중치 w^[l]이 단위행렬보다 조금 더 크다면 매우 깊은 네트워크의 경우 활성값 폭발<br>
가중치 w^[l]이 단위행렬보다 조금 작다면 매우 깊은 네트워크의 경우 활성값 기하급수적감소<br>
<br>
비슷한 주장으로 미분값(경사 하강법에서 계산하는 경사가 층의 개수에 대한 함수)도 기하급수적으로 증가하거나 감소한다<br>
<br>
현대의 신경망은 보통 L = 150<br>
이런 깊은 신경망에서 활성값이나 경사가 L에 대한 함수로 기하급수적으로 증가하거나 감소한다면 값들은 아주 커지거나 작아질 수 있다<br>
-> 훈련을 시키는 것이 어려워진다<br>
<br>
3) 심층 신경망의 가중치 초기화<br>
가중치를 어떻게 초기화시키느냐에 따라 문제 해결할 수 있다!<br>
<br>
예제) 단일 뉴런에 대한 가중치 초기화<br>
<br>
특성 4개 <br>
깊은 망에서 입력은 a^[l]인 어떤 층이 된다<br>
z = w1x1 + w2x2+ ... + wnxn + b<br>
b값은 무시<br>
z의 값이 너무 크거나 작아지지 않도록 만들어야 한다<br>
n의 값이 클수록 wi의 값이 작아져야 한다<br>
<br>
1) wi의 분산을 1/n으로 설정한다 (n : 입력 특성의 개수) <br>
실제로 특정 층에 대한 가중치 행렬 w^[l]<br>
w^[l] = np.random.randn(shape) * np.sort(1/n^[l-1])<br>
2) ReLU 활성화 함수를 사용하는 경우 wi의 분산을 2/n^[l-1]으로 설정한다<br>
w^[l] = np.random.randn(shape) * np.sort(2/n^[l-1])<br>
n^[l-1]을 사용하는 이유 : 층 l은 해당 층의 각 유닛에 대해 n^[l-1]의 입력을 갖는다<br>
따라서 입력 특성 혹은 활성값의 평균이 대략 0이고 표준 편차 1을 갖는다면 이것 역시 비슷한 크기를 갖게 된다<br>
3) tanh 활성화 함수를 사용하는 경우 wi의 분산을 1/n^[l-1] 또는 2/n^[l-1]+n^[l]으로 설정한다<br>
각각의 가중치 행렬 w를 1보다 너무 커지거나 너무 작아지지 않게 설정해서 너무 빨리 폭발하거나 소실되지 않게 한다<br>
식들은 가중치 행렬의 초기화 분산에 대한 기본 값을 제공할 뿐이다<br>
이와 같은 분산을 원한다면 분산 매개변수를 하이퍼파라미터로 조정해야한다<br>
<br>
4) 기울기의 수치 근사<br>
깊은 네트워크를 훈련시킬 때 신경망을 더 빨리 훈련시키는 기법<br>
경사 검사 : 역전파를 맞게 구현했는지 확인하는데 도움을 준다<br>
<경사의 계산을 수치적으로 근사하는 방법><br>
<br>
f(θ) = θ^3<br>
1) θ = 1인 경우 <br>
θ+ϵ, θ-ϵ을 구하면 각각 1.01, 0.99<br>
ϵ = 0.01<br>
<br>
작은 삼각형과 큰 삼각형에서 너비 분의 높이 구한다<br>
<br>
큰삼각형의 너비 분의 높이 : f(θ+ϵ) - f(θ-ϵ) / 2ϵ<br>
<br>
(1.01)^3 - (0.99)^3 / 2(0.01) = 3.0001<br>
g(θ) = 3θ^2 = 3 (θ=1) <br>
근사 오차 = 0.0001<br>
<br>
작은삼각형의 너비 분의 높이 : f(θ+ϵ) - f(θ) / ϵ<br>
(1.01)^3 – 1 / 0.01 = 3.0301<br>
근사 오차 = 0.03 더 크다!!<br>
<br>
따라서 도함수를 근사하기 위해 양 쪽의 차이를 이용하는 방법을 사용하면 3에 매우 가까운 값이 나온다<br>
g(θ)가 f의 도함수에 대한 더 올바른 구현이다<br>
한 쪽의 차이만을 사용하는 것보다 두 배는 느리게 실행되지만 훨씬 더 정확하다<br>
<br>
f​′(θ)=limϵ→inf f(θ+ϵ) - f(θ-ϵ)/2ϵ<br>
0이 아닌 ϵ에 대해서 근사의 오차는 O(ϵ^2) 이다. ϵ은 굉장히 작은 수이다.<br>
빅오 표기법은 오차가 상수라는 것을 나타낸다<br>
​​근사 오차 예시이므로 O(ϵϵ^2)를 1이라고 생각하자<br>
<br>
f​′(θ)=limϵ→inf f(θ+ϵ) - f(θ)/ϵ<br>
0이 아닌 ϵ에 대해서 근사의 오차는 O(ϵ) 이다.<br>
​​ϵ이 1보다 작은 값이므로 ϵ^2보다 훨씬 큰 값이다 -> 훨씬 덜 정확한 근사이다<br>
<br>
f​′(θ)=limϵ→inf f(θ+ϵ) - f(θ-ϵ)/2ϵ처럼 양 쪽 차이를 사용하는 것이 더 정확하다!<br>
<br>
5) 경사 검사<br>
경사 검사 : 시간을 절약하고 역전파의 구현에 대한 버그를 찾는데 도움을 준다<br>
<br>
신경망은 매개변수 W^[1], b^[1]부터 W^[L], b^[L]까지 가지고 있다<br>
1. 매개변수들을 하나의 큰 벡터 θ로 바꾼다<br>
행렬 W^[1]을 벡터로 크기를 바꾼다<br>
모든 W행렬을 받아서 벡터로 바꾸고 모두 연결시킨다<br>
매우 큰 벡터 매개변수 θ를 얻게 된다<br>
비용 함수는 J(W, b) 에서 J( θ ) 로 변한다<br>
<br>
마찬가지로 dW^[1], db^[1],...,dW^[L], db^[L]의 매개변수를 매우 큰 벡터 dθ로 만든다<br>
dW^[1] 행렬을 벡터로 바꾼다<br>
db^[1]은 이미 벡터이다<br>
크기를 바꾸고 연결해 모든 미분값을 매우 큰 벡터 dθ로 바꿀 수 있다<br>
<br>
dθ가 비용함수 J( θ )의 기울기인가?<br>
<br>
Gradient checking (Grad check)<br>
J( θ ) = J( θ1, θ2, θ3, ... )<br>
for each I :<br>
	dθapprox[i] = J( θ1, θ2, ..., θi + ϵ,...) - J( θ1, θ2, ..., θi - ϵ,...) / 2ϵ ≈ dθ[i]<br>
	(dθi가 비용함수 J의 도함수라면, 함수 J의 θi에 대한 편미분과 같다)<br>
	dθapprox ≈ dθ  <br>
두 벡터가 꽤 가까운지 알아봐야 한다 -> 두 벡터의 유클리드 거리 계산<br>
dθapprox – dθ의 L2 노름을 구한다<br>
<br>
벡터가 아주 작거나 큰 경우에 대비해 분모가 이 식을 비율로 바꾼다<br>
보통 거리가 10^-7보다 작으면 잘 계산되었다고 판단한다<br>
원소의 차이가 너무 크면 버그가 있을 수 있다<br>
10^-3보다 큰 값이면 버그가 너무 크니 θ의 개별적인 원소를 신중하게 살펴서 <br>
특정 i에 대해 dθapprox[i]와 dθ[i]의 차이가 심한 값을 추적해서 미분의 계산이 옳지 않은 곳이 있는지 확인해야 한다<br>
<br>
6) 경사 검사 시 주의할 점<br>
1. 훈련에서 경사 검사를 사용하지 말고 디버깅을 위해서만 사용해야 한다<br>
-모든 I의 값에 대한 dθapprox[i]를 계산하는 것은 매우 느리기 때문이다<br>
디버깅할 때만 경사 검사를 구현하기 위해 dθ를 계산하는 역전파를 이용해 도함수를 계산하고 dθ에 가까워지게 한다. 과정이 끝나면 경사 검사를 끄고 모든 반복마다 실행되지 않도록 한다.<br>
2. 알고리즘이 경사 검사에 실패 했다면, 어느 원소 부분에서 실패했는지 찾아본다. <br>
dθapprox가 dθ에서 매우 먼 경우 서로 다른 I에 대하여 어떤 dθapprox[i]의 값이 dθ[i]의 값과 매우 다른지 확인한다. <br>
예를 들어 어떤 층에서 θ나 dθ의 값이 대응되는 db^[l]과 매우 멀지만 대응되는 dw^[l]과는 매우 가까운 경우 - θ의 서로 다른 컴포넌트는 b나 w의 다른 컴포넌트에 대응된다. 이 경우에는 db를 어떻게 계산하느냐에 따라서 버그가 발생할 것이다.<br>
dθapprox가 dθ에서 매우 멀고, 모든 컴포넌트가 dw 혹은 특정한 층의 dw에서 온 것을 발견한다면 버그의 위치를 알아내는데 도움을 받을 수 있다
3. 경사 검사를 할 때 사용하는 정규화 항을 기억해라.<br>
<br>
비용함수 J( θ ) = 1/m * (손실함수의 합) + 정규화 항<br>
dθ는 θ에 대응하는 J의 경사로 정규화 항을 포함<br>
4. 경사 검사는 드롭아웃에서는 작동하지 않는다<br>
드롭아웃은 모든 반복마다 은닉 유닛의 서로 다른 부분집합을 무작위로 삭제하기 때문에 적용하기 쉽지 않다. 비용함수 J는 어떤 반복에서든지 삭제될 수 있는 기하급수적으로 큰 노드의 부분집합으로 정의되기 때문에 비용함수 J를 계산하는 것이 매우 어렵다. 따라서 드롭아웃을 이용한 계산을 이중으로 확인하기 위해 경사 검사를 사용하기는 어렵다<br>
따라서 교수님은 주로 드롭아웃 없이 경사 검사를 구현한다. 드롭아웃의 keep_prop을 1.0로 설정하고 드롭아웃을 킨다. <br>
추천하는 방법 : 드롭아웃을 끄고 알고리즘이 최소한 드롭아웃 없이 맞는지 확인하고, 다시 드롭아웃을 킨다.<br>
5. 거의 일어나지 않지만 가끔 무작위 초기화를 해도 초기에 가까울 때 경사 검사가 잘 되는 경우가 있다. 그러나 경사 하강법을 실행하면 w와 b는 점점 커진다. 역전파의 구현이 w와b가 0에 가까울 때만 맞는 것일 수 있다. w와 b가 커지면 그 값은 더 부정확해진다. <br>
따라서 무작위적인 초기화에서 경사 검사를 실행하고 네트워크를 잠시 동안 훈련해서 w와 b가 0에서 멀어질 수 있는 시간을 준다. 일정 수의 반복을 훈련한 뒤에 경사 검사를 한번 더 실행시킨다. <br>
