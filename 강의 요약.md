# 강의 요약

## ****자연어 처리 활용 분야와 트렌드****

- 자연어 처리 발전 동향
    - 기본적으로 컴퓨터가 주어진 단어나 문장, 문단, 글을 이해하는 Natural language understanding, Natural language generation 두 분야로 구성됨
    - 본 과정에서는 자연어처리에 관련된 deep learning 기술과 구체적인 application, 다음 단어를 예측하는 language modeling, 기계 번역 등을 다루게 된다

---

- ACL, EMNLP, NAACL 의 학회에서 신기술 동향을 파악할 수 있음
    1. low-level parsing
        - 문장의 각 단어를 정보 단위로 받아들임(token), 주어진 문장을 단어 단위로 쪼개나가는 과정 (tokenization) → token의 sequence를 문장으로서 받아들인다
        - 단어의 어근을 추출하는 것 → stemming
    2. word and phrase level
        - 하나의 고유 명사로서 인식하는 과정 (Named entity recognition(NER)), part-of-speech(POS) tagging, noun-phrase chunking, dependency parsing 등의 과정이 있음
    3. Sentence level
        - 문장의 긍, 부정을 판단하는 Sentiment analysis
        - 기계 번역 machine translation
    4. Multi-sentence and paragraph level
        - 문단에 대한 요약, 질문에 대한 응답 등의 task가 진행됨

---

- text mining : 빅데이터 분석과 관련, 특정 키워드의 빈도수나 트렌드를 분석하여 정보를 파악, 상품에 대한 리뷰를 분석하여 키워드 분석 → 소비자 반응을 얻어낼 수 있음
- 각기 다른 키워드임에도 같은 의미일 수 있으므로 이에 대해 grouping이 필요
- 해당 분야는 사회과학 분야와도 밀접한 관련이 있음, 사회과학적인 인사이트 발굴에 도움이 된다/ social media data를 수집하여 신조어를 파악할 수 있고 이는 현대 사회 현상과 관련이 있음을 분석할 수 있다.
- KDD, WWW, WSDM, CIKM, ICWSM 등의 세계 학회에서 신기술 트렌드 파악 가능

---

Information retrieval : 검색 기술 연구 / 최근 검색 시스템의 성능이 고도화되면서 어느정도 성숙한 상태에 이르렀음을 알 수 있다.

- 해당 분야와 관련된 추천 시스템 도메인 기술을 이해하고 사용자에 대한 적절한 추천을 제시할 수 있다. 사용자가 필요로하는 상품, 음악, 미디어에 대한 검색을 자동화하므로써 사용될 수 있다.

---

현재 강의에선 자연어 처리를 주로 다룬다.

Trends of NLP : 자연어 처리 분야의 발전

CV분야의 기술을 가져오기 위해 자연어 (문장)을 각각의 단어로 구분하여 벡터화하는 과정이 필요로 하게 된다 (word embedding) 

ex : i love this movie → i / love / this / movie 로 하나의 sequence로 볼 수 있음 (순서가 뒤바뀌면 다른 뜻으로 인식해야한다 ) 

sequence를 처리할 수 있는 모델로써 RNN, LSTM이 등장 → Attention

기존의 자연어 처리 Module을 self - attention으로 바꾸며 큰 발전을 가져옴

번역과 같은 분야에서는 각 언어의 사용 패턴을 대응하는 과정이 필요함 → RNN 기반의 sequence data를 학습 데이터로 사용하여 기계 번역 능력의 향상을 가져옴

→ transform의 등장으로 자연어처리 뿐만아니라 영상처리, 신약 개발 등의 다양한 분야에서 활용

각 task에 특화된 deep learning module이 존재했으나, 지금은 self-attention(자가지도학습)을 기반으로 구성된 transformer 모델이 범용적으로 활용되고, 특정 task에 전이 학습의 형태로 사용

[자연어 처리의 모든 것](https://m.boostcourse.org/ai330/lecture/1455360)

## 기존의 자연어 처리 기법

deep learing 기술이 적용되기 이전에 활용되던 bag-of-words의 표현형과 naiveBayes Classifier에 대해 알아본다

---

1. Bag-Of-Words
    1. 중복 단어를 제거하여 사전에 등록
    2. categorical variable로 각각의 단어를 등록, 각 word를 one-hot-vector로 표현 ( 단어의 의미와 상관없이 모두 동일한 관계값을 갖도록 가중치를 설정할 수 있음)
    3. 각 문장은 각 one-hot-vector를 더하여 표현 ( Bag-ofWords Representation)
2. NaiveBayes Calssifier
    1. C개의 category가 있을 때 각 카테고리에 포함될 확률을 정의
    2. 가장 높은 c를 갖는 category를 설정하여 분류

---

예시)

![Untitled](자료/Untitled.png)

각각의 문장에 해당하는 단어를 CV, NLP로 카테고리를 나눈 후, test 문장의 단어가 어느 category에 더 많이 포함되는지의 확률을 판단하여 Class를 구분한다.

1/카테고리 수 * 각 단어가 속하는 곳에 category에 포함될 확률

[자연어 처리의 모든 것](https://m.boostcourse.org/ai330/lecture/1455361)

## ****Word Embedding - (1)Word2Vec****

- 각 단어들을 한 점의 좌표로 나타내는 vector로 나타내는 기술 (각 차원의 한 좌표에 단어를 할당하는 것으로 볼 수 있다)
- **각 단어의 연관성을 따져서 연관성이 깊으면 좌표상으로 인근에 위치시킨다**

---

**word2vec**

같은 문장에서 인접한 위치에 존재하는 단어들 간에 연관성이 있을 것이라고 가설 → 이를 기반으로 좌표상으로 유사한 위치의 좌표를 부여한다.

문장의 단어의 갯수만큼에 Input, Output 벡터 사이즈를 입력/출력. 이 때 연산에 사용되는 히든 레이어(hidden layer,은닉 층)의 차원(dim)은 파라미터

실제로 Tensorflow나 Pytorch와 같은 프레임워크에서는 임베딩 레이어와의 연산은 0이 아닌 1인 부분, 예를 들어 [0,0,1]의 벡터인 경우는 3번째 원소와 곱해지는 부분의 컬럼(column)만 뽑아서 계산

inf값으로 지정하여 양의 무한대일 경우 softmax를 지나면 1로 음의 무한대는 0으로 하여 유사도가 있는 경우 유사도를 최대한 크게, 유사도를 최대한 작게하여 word2vec의 과정을 지난다

hidden layer와의 내적을 통해 w2, w1 간의 내적값이 최대한 커지도록 학습이 진행된다

유사한 형태를 갖는 vectorization을 통해 내적값이 최대로 되는 것을 알 수 있다.

---

![Untitled](자료/Untitled%201.png)

학습된 결과에 대한 vector를 도식화 → 유사한 관계 간의 벡터의 차는 비슷한 것을 볼 수 있다.

---

**Application of Word2Vec**

- Machine translation
- Sentiment analysis
- Image Captioning

[자연어 처리의 모든 것](https://m.boostcourse.org/ai330/lecture/1455362)

## ****Word Embedding - (2)GloVe****

Glove는 두 단어가 한 windown내에서 사전에 미리 등장 빈도수를 계산하여, 단어 간의 내적값과 사전에 ㄱ산된 값의 차이를 줄여가는 형태로 학습한다.

Word2Vec는 모든 연산을 반복하지만, Glove는 사전에 계산된 Ground Truth를 사용해 반복계산 줄인다

---

**사전 학습된 Glove 모델**

- 사전 학습된 모델은 위키피디아 데이터를 기반으로 하여 6B token만큼 학습되었으며, 중복 제거 시의 단어 수가 40만개
- 학습된 모델을 나타낼 때 뒤에 붙는 "uncased"는 대문자 소문자를 구분하지 않는다는 의미이며, 반대로 "cased"는 대소문자를 구분한다는 의미

[자연어 처리의 모든 것](https://m.boostcourse.org/ai330/lecture/1455363)