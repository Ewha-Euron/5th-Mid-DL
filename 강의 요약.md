# 강의 요약

## ****Recurrent Neural Network (RNN)****

- RNN

![Untitled](자료/Untitled.png)

현재 타임스텝에 대해 이전 스텝까지의 정보를 기반으로 예측값을 산출하는 구조의 딥러닝 모델

매 타임스텝마다 동일한 파라미터를 가진 모듈을 사용 → 재귀적인 호출의 특성(RNN)

- 계산방법

![Untitled](자료/Untitled%201.png)

![Untitled](자료/Untitled%202.png)

time step마다 h_t를 이용해서 hidden state를 다시 구한다

이 때, W와 입력값으로 tan_h를 곱해서 h_t를 구한다

구해진 h_t, x_t를 입력으로 y_t를 산출

- RNN의 종류

![Untitled](자료/Untitled%203.png)

one to one : 정보를 입력값으로 classifiaction

one to many : 하나의 이미지에 대해 설명글을 작성하는 task 

many to one : 문장을 넣으면 label에 대해 classification

many to many : 입력값을 끝까지 다 읽은 후, 번역된 문장을 출력

many to many : 각 단어의 품사에 대해 태깅하는 POS, 영상의 프레임 레벨에서 각각 예측하는 task가 해당

## ****Character-level Language Model****

- 자연어 처리의 가장 간단한 task
- 기본적으로 문자열, 단어들의 순서를 바탕으로 다음 단어를 예측하는 task
- many to many를 사용

one hot vector형태의 입력으로 단어를 input으로 주어지면 h_t값을 구함

선형 변환을 고려하는 함수 + bias까지 고려 + 비선형 변환 tanh까지 고려

이때 각 타임스텝별로 output layer를 통해 차원이 4(유니크한 문자의 개수) 벡터를 출력해주는데 이를 logit이라고 부르며, softmax layer를 통과시키면 원-핫 벡터 형태의 출력값이 나옴

## ****Backpropagation through time and Long-Term-Dependency****

모든 정보를 담기는 어렵기 때문에 부분 메모리만 사용해서 학습을 진행 (Truncation)

BPTT란, Backpropagation through time의 줄임말로 RNN에서 타임스텝마다 계산된 weight를 backward propagation을 통해 학습하는 방식을 의미

기존의 vanilla RNN으로는 학습이 어려운데 gradient 전파 과정에서 vanishing이나 explosion이 일어나기 때문(Long-Term_Dependency) → 이를 해결하기 위해 LSTM 모델로 학습

---

time step이 3인 RNN의 BPTT

![Untitled](자료/Untitled%204.png)

- 3번째 time step의 hidden state 인 h_3*h*3 를 h_1*h*1 으로 표현하면 맨 아랫줄의 식과 같이 표현
BPTT를 통해 gradient를 계산해주면, tanh 로 감싸진 괄호안의 값들 중에 3 값이 속미분되어 계수로 쓰임
n이 3이 아닌 발산할 경우 무한하게 값이 커지는 것을 알 수 있고, Gradient Vanishing/Exploding 문제가 발생하게 된다.

## ****Long Short-Term Memory (LSTM)****

![Untitled](자료/Untitled%205.png)

단기 기억으로 저장하여 이걸 때에 따라 꺼내 사용함으로 더 오래 기억할 수 있도록 개선
Cell state에는 핵심 정보들을 모두 담아두고, 필요할 때마다 Hidden state를 가공해 time step에 필요한 정보만 노출하는 형태로 정보가 전파

![Untitled](자료/Untitled%206.png)

- I : Input gate, cell에 사용할지 말지 결정. 들어오는 input에 대해서 마지막에 sigmoid를 거쳐 0-1 사이 값으로 표현. 이 값은 cell state와 hidden state 두 갈래로 흐르게 됩니다.

![Untitled](자료/Untitled%207.png)

- f : Forget gate 로 불리며, 정보를 어느정도로 지울지를 0~1사이의 값으로 표현

![Untitled](자료/Untitled%208.png)

- o : Output gate로 불리며, Cell 정보를 어느정도 hidden state에서 사용해야할 지를 0~1사이 값으로 표현

![Untitled](자료/Untitled%209.png)

- g : Gate gate로 불리며, 어느정도로 Cell state에 반영해야할 지를 -1 ~ 1 사이의 값으로 표현

![Untitled](자료/Untitled%2010.png)

---

**LSTM은 각 time step마다 필요한 정보를 단기 기억으로 hidden state에 저장하여 관리되도록 학습하는 것으로 역전파 진행시 가중치(W)를 곱해주는 연산이 아닌, forget gate를 거친 값에 대해 필요로 하는 정보를 덧셈을 통해 연산하여 gradient vanishing/exploding문제를 방지**

---

- GRU : Gated Recurrent Unit

![Untitled](자료/Untitled%2011.png)

LSTM의 경량화 버전, 전체적인 동작원리는 유사하지만, Cell state, Hidden state를 일원화하여 경량화한 모델

forget gate 대신 1-input gate를 사용하여 h_t를 구할 때 가중평균의 형태로 계산

계산량과 메모리 요구량을 LSTM에 비해 줄여준 모델이면서 동시에 성능면에서도 LSTM과 비슷하거나 더 좋은 성능을 내는 모델