# 강의 요약

## ****배치 정규화****

- 배치 정규화 : 하이퍼파라미터의 탐색을 손쉽게 만들고, 신경망과 하이퍼파라미터의 상관관계를 감소시킨다
    - 깊은 신경망도 손쉽게 학습할 수 있도록 도움
    
    ex. logistic regression에서의 정규화에 대한 효과 → 경사 하강법에 적합한 형태의 등고선으로 변환 (입력 변수에 대한 정규화를 통해) 
    
    - 이 때 깊은 신경망의 경우 은닉층의 활성값까지 정규화가 가능할지? (다음 층에 영향을 주므로) → 배치 정규화! (활성함수 이전의 값을 정규화하는 것이 일반적이다)
- 구현 방법
    - Given some intermediate values in NN → z^(1)~z^(m)까지의 값이 있다.
    - z에 대한 평균과 분산을 계산 ( 표준편차가 0일 경우를 대비해 분모에 시그마를 더해준다) 이를 통해 평균이 0 분산이 1이 되도록 만든다. → 그러나 모든 은닉 유닛이 같은 평균과 분산을 갖는 것은 좋지 않으므로 아래와 같은 값을 정의한다 (표준화된 평균과 분산에서 특정 평균과 분산을 갖도록 정규화한다)
        
        ![Untitled](%E1%84%80%E1%85%A1%E1%86%BC%E1%84%8B%E1%85%B4%20%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%A3%E1%86%A8%2084d175b0577a49f7b1fe6241891dc3b8/Untitled.png)
        
    - *γ*  와  *β*  는 학습과정에서 학습하는 파라미터로 이를 조절해서 은닉층이 각각 다른 평균과 분산을 갖도록 할 수 있다. → 입력값과의 차이 (입력값은 평균0 분산1을 갖도록 조정하지만 은닉층은 다르도록 조절함 → 활성화함수의 비선형을 살릴 수 있도록)

## ****배치 정규화 적용시키기****

- hidden layer = z + a(activated value)
- input 값에 대해서 w,b로 z값 구하기 → *γ*  와  *β*에 대해서 z(tilda)값 구하기 → 이에 대해 활성화함수 적용
- tf.nn.batch_normalization으로 구현 가능
- 배치 정규화는 훈련 집합의 미니 배치에 적용된다 → 각 정규화마다 해당 미니 배치의 데이터만을 이용해서 정규화를 진행. 이 때 미니 배치의 연산에서 평균을 빼주는 과정이 있으므로 b^[l]항이 사라지게 된다

<aside>
❗ **알고리즘**
for t=1~ num Mini Batches
compare forward prop. 
In each hidden layer, use BN to swap z → z(tilda)

</aside>

## ****배치 정규화가 잘 작동하는 이유는 무엇일까요?****

- input을 정규화하는 것이 왜 잘 작동하는 걸까? → 입력 특성 x에 대해 비슷한 범위를 갖도록하여 학습 속도를 높인다 → 비슷한 일을 하기때문에 성능이 올라간다?
- 깊은 신경망일 수록 input에 대한 영향이 적어진다
- 공변량 변화 → x의 분포가 바뀌면 다시 학습을 시도해야한다. 앞의 값에 따라 은닉층이 진행됨에 따라 계속해서 hidden layer의 앞의 값이 변화하고 있다 → 따라서 배치 정규화는 앞의 값의 변화 정도를(평균과 분산이 일정하도록) 제한해 뒤쪽 layer가 학습할 때 유리하도록 돕는다.
- 배치 정규화의 또 다른 효과 → 파라미터의 정규화(regularization)입니다. 미니배치로 계산한 평균과 분산은 전체 데이터의 일부으로 추정한 것이기 때문에 노이즈가 존재
    
    드롭아웃의 경우 은닉유닛에 확률에 따라 0 혹은 1을 곱하기 때문에 곱셈에 대한 노이즈가 있다. 배치 정규화의 경우 곱셈과 덧셈 모두에 대한 노이즈가 존재한다. 따라서 약간의 정규화 효과가 있다.
    
    → 노이즈를 추가하여 은닉층이 하나의 은닉 유닛에 너무 의존하지 않도록한다
    

## ****테스트시의 배치 정규화****

- 테스트는 배치가 하나이므로 평균과 분산을 계산할 수 없다 따라서, 학습시에 사용된 미니배치들의 지수 가중 이동 평균을 추정치로 사용한다.