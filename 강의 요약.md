# 강의 요약

## ****왜 케이스 스터디를 하나요?****

신경망의 종류를 살펴보는 이유?

컴퓨터 비전 분야의 연구 대상은 구성 요소들을 조합하여 효과적인 합성곱 CNN을 만드는 방법, 다른 사람의 모델에서 영감을 얻는 것이 중요하다

누군가 발견해낸 신경망의 구조가 좋은 성능을 보인다면 특정 분야에서도 좋은 성능을 보일 확률이 높다

---

Classic networks : 현대 컴퓨터 비전의 기반을 마련함

1. LeNet-5
2. AlexNet
3. VGG

---

ResNet (152개의 층을 훈련)

---

Inception Net(Google Net)

→ 위의 예시 모델들을 이용해서 어떻게 효과적인 합성곱 신경망을 구축할까?

## ****고전적인 네트워크들****

**LeNet-5 : 손글씨의 숫자를 인식**

(gray scale → 1차원을 가짐)

![Untitled](자료/Untitled.png)

avg pooling을 더 많이 사용

softmax 함수를 사용하지 않음

60k의 변수를 가짐 (적은 편)

신경망의 층이 깊어질 수록 너비와 높이는 감소하고 채널 수는 증가한다

최근엔 conv + pooling이 함께 쓰이기도 한다

당시에는 ReLU를 사용하지 않고, sigmoid와 tanh를 사용한다, 컴퓨터 계산 능력이 느려서 각각의 필터가 서로 다른 채널에 적용되었다. (변수의 수를 줄이기 위해)

---

**AlexNet : 227x227x3의 이미지를 사용, 실제 논문에서는 224x224x3을 이용 (colored image)**

![Untitled](자료/Untitled%201.png)

LeNet보다 더 간단하지만 더 큰 신경망  60M개의 변수를 갖는다

ReLU 활성화 함수를 사용

Multiple GPU를 사용해서 학습

---

**VGG-16 : 많은 hyper parameter 대신 s=1인 3x3크기의 고정된 filter를 사용하고, 2x2의 max-pooling을 사용 / 가중치를 가진 층이 16개 있다는 것을 의미**

![Untitled](자료/Untitled%202.png)

모든 필터의 크기가 3x3을 갖고, 1의 stride를 통해 동일한 크기의 결과를 생성, 2x2 max-pooling을 사용할 때만 크기가 변화한다.

훈련시킬 변수의 개수가 많아 네트워크의 크기가 커진다는 단점이 있다.

## ****ResNets****

**Residual block : gradient descent를 막는 방법**

![Untitled](자료/Untitled%203.png)

![Untitled](자료/Untitled%204.png)

기존에는 활성값에서 → Wa+b를 통해 선형화 후 → ReLU 비선형 함수를 적용하여 다음 값을 얻는다

main path를 따르는 것이 아닌 오른 쪽의 식처럼 ReLU를 적용하기 전 기존의 a^[l]을 더해서 short cut을 따른다.(skip connection) 해당 값이 residual block값이 된다.

오른쪽과 같은 residual block을 사용하면 깊은 신경망 학습이 가능해진다

![Untitled](자료/Untitled%205.png)

여러 개의 residual block을 가진 평형망에 Skip connection을 더해주어서 ResNet을 구현한다. 

![Untitled](자료/Untitled%206.png)

일반적인 경우 gradient descent로 인한 training error가 감소하다 증가하는 모양을 보이지만 ResNet에서는 이론과 비슷한 모양의 training error graph를 확인할 수 있다.

## ****왜 ResNets 이 잘 작동할까요?****

네트워크가 깊어질 수록 훈련 세트를 다루는 데에 지장이 생긴다 → 깊은 네트워크를 선호하지 않는 경향이 생김 그러나 ResNet의 경우는 다름

![Untitled](자료/Untitled%207.png)

simple connection에 의한 출력값에 대한 합을 정리하면 위와 같은 항등식을 갖는다. 이로 하여금 스킵 연결을 통해 두 층이 없는 더 간단한 항등식을 학습하여 더 좋은 성능을 낼 수 있게 만든다. 이 과정에서 z^[l+2]와 a^[l]은 같은 차원을 가져야하므로 a^[l]에 W_s 행렬을 이용해 같도록 만든다.

신경망 전반에 걸쳐서 ReLU를 사용, 입력값을 제외하고는 모든 활성값이 0보다 크다

추가된 층이 항등 함수를 학습하기 용이하기 때문에 깊은 층을 가지고도 학습이 잘 되거나 성능이 향상한다. 기본적으로 성능에 손해가 없고 경사 하강법으로 점점 발전함

simple connection에서의 합에서 가정은 차원이 같다는 것, 따라서 a^[l+2]와 a^[l]의 차원은 같도록 만드는 것이 중요하다. 두 개의 동일한 차원의 벡터의 합을 구하기 위해서 W_s라는 행렬을 추가하여 a^[l]과 a^[l+2]가 같도록 한다.

![Untitled](자료/Untitled%208.png)

## ****Network 속의 Network****

1x1 convolution와 합성곱을 진행하는 것이 왜 유용할까?

multiple channel에서의 1x1 합성곱은 각 필터에 대해 적절한 채널 수만큼의 수치를 합성곱에 사용하고 ReLU 비선형성을 사용한다

여러개의 유닛에 대한 입력을 한 조각으로 묶는 셈이되고 출력은 필터의 수만큼 갖게된다. 

---

![Untitled](자료/Untitled%209.png)

높이와 너비를 줄이고 싶을 때 채널의 수가 너무 크다면 pooling 대신 filter의 수를 줄이고 싶은 채널 수만큼으로 합성곱을 진행하면 된다.

## ****Inception 네트워크의 아이디어****

인셉션 네트워크의 핵심개념을  알아본다

![Untitled](자료/Untitled%2010.png)

위와 같이 1x1 filter를 이용해서 변수의 크기로 줄이는 작업을 진행한다. 이로인한 변수의 크기를 조절할 수 있다. 여기서 사용된 1x1 conv layer를 병목층이라고도 부른다. 적절하게 구현 시 정보에 대한 손실 없이 성능에 영향을 주지 않을 수 있으며 계산량을 줄일 수 있다.

필터의 크기나 풀링을 결정하는 대신 전부 다 적용해서 출력들을 합친 뒤 네트워크로 하여금 스스로 변수나 필터 크기의 조합을 학습하게 만드는 것

## ****Inception 네트워크(Google Net)의 아이디어****

![Untitled](자료/Untitled%2011.png)

각각 필터를 통해 다양한 크기로 줄인 후 다시 concat하여 만드는 것

위의 Inception Module을 이용해서 Inception network를 구현한다.

![Untitled](자료/Untitled%2012.png)

아래의 곁가지는 은닉층을 가지고 예측을 하는 과정을 거침 (각각에 대한 softmax처리를 진행)