# 강의 요약

## 입력값의 정규화

### 신경망의 훈련 속도 가속을 위한 정규화

- 2차원의 입력 세트가 있을 때 → 두 단계에 따른 입력 정규화
    
    정규화 기법 (평균을 0으로, 분산을 1로)
    
    train set과 test set을 같은 *μ*,*σ* 를 사용해야 한다.
    
- 정규화를 해야하는 이유

![Untitled](자료/Untitled.png)

![Untitled](자료/Untitled%201.png)

정규화 이전의 cost function에선 경우에 따라 최솟값을 찾기 위해 많은 수의 iteration이 필요할 수 있지만 정규화된 cost function에선 어느 방향으로든 최적화할 때 비슷한 수의 iteration 수를 갖는다.

모델 정확도에 영향을 주진 않기 때문에 정규화를 하는 것을 추천

## 경사소실/경사폭발

- vanishing gradients
    
    w가 단위행렬보다 작은 값의 경우 매우 깊은 네트워크의 경우 activation의 값이 매우 작아져서 update가 이루어지지 않는 문제가 발생
    
- exloding gradients
    
    w가 단위행렬보다 큰 값의 경우 activation의 값이 폭발적으로 거대해진다.
    

→ 이 때문에 가중치 초기화 값을 신중하게 정하는 것이 중요

## 심층 신경망의 가중치 초기화

- 단일 신경망에서

![Untitled](자료/Untitled%202.png)

z= w*i의 합일 때 w의 분산을 1/n으로 설정하여 각 항이 점점 작아지도록 설정

```python
# ReLU (가장 일반적인 활성화 함수)
W[l] = np.random.randn(shape of matrix) * np.sqrt(2/n[l-1])

# ReLU의 경우 sqrt(2/n[l-1])로 두는 것이 더 유용함
# n[l-1]로 설정하는 이유는 일반적인 유닛에 대해 n-1개의 입력값을 갖기 때문
```

입력 특성의 평균이 대략 0, 표준 편차가 1일 경우 비슷한 값을 가지며 gradient vanishing, exploding에 영향을 줄 수 있다.

```python
# tanh(x)
W[l] = np.random.randn(shape of matrix) * np.sqrt(1/n[l-1])

# tanh의 경우 sqrt(1/n[l-1])로 두는 것이 더 유용함
# or sqrt(2/(n[l-1]+n[l]))
```

## 기울기의 수치 근사

- gradient checking을 이용한 미분 계산 (역전파를 맞게 구현했는지 확인 가능)

## 경사 검사(gradient checking)

- W행렬을 받아서 벡터로 바꾼 매개변수 세타를 얻는다
- 비용함수 J를 w,b 대신 세타에 대해 나타낸다
    
    그 후 수치 미분을 구한다
    
    ![Untitled](자료/Untitled%203.png)
    
    유클리디안 거리를 사용해서 유사도를 계산하고 그 값이 10^-7보다 작으면 잘 계산되었다고 판단한다.
    

## 경사 검사 시 주의할 점

- only use to debug  - 속도가 매우 느리기 때문에
- 특정 부분에서의 버그는 그 경사가 계산된 층에서 생긴 것
- 정규화 항도 함께 경사 검사 계산에 포함해야 한다
- dropout은 끄고 사용한다
- 경사 검사가 초기부터 잘 되는 경우 → 조금 훈련시킨 후 다시 사용해보기