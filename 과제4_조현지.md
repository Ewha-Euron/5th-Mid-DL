5. 심층 신경망 네트워크<br>

1) 더 많은 층의 심층 신경망<br>
   로지스틱 회귀 – 한 층의 신경망<br>
   은닉층의 개수 값을 변경해 정확도를 평가한다<br>
   <표기법><br>
   •  L   : 네트워크 층의 수<br>
   •  n ​^^[l ]​​ : l층에 있는 유닛 개수<br>
   •  a ​[l ]​​ : l층에서의 활성값<br>
   •  a ​[0 ]​​ : 입력 특징 (X)<br>
   •  a ​[L ]​​  : 예측된 출력값 ( ​y ​^​​ )<br>
   <br>
2) 정방향전파와 역방향전파<br>
   <정방향전파><br>
   input : a^[l-1]<br>
   output : a^[l], cache(z^[l])<br>
   a^[0] : 한 번에 하나씩 할 경우의 학습 데이터에 대한 입력 특성<br>
   A^[0] : 전체 학습 세트를 진행할 때의 입력 특성 = 체인에서 첫 번째 정방향 함수에 대한 입력값<br>
   과정을 반복하면 왼쪽에서 오른쪽으로 가는 정방향 전파를 계산하는 것<br>
   <역전파><br>
   input : da^[l]<br>
   output : da^[l-1], dW^[l], db^[l]<br>
   <요약><br>
   입력X -> ReLU를 활성화 함수로 갖는 첫 번째 층 -> 또다른 ReLU를 활성화 함수로 갖는 두 번째 층 -> sigmoid를 활성화 함수로 갖는 세 번째 층(이진 분류) -> ​y ​^-> L(y ​^,y ​)))))) -> 역전파 과정(도함수 계산)dW^[3], db^[3], (da^[2] 옮김)-> dW^[2], db^[2], (da^[1] 옮김) -> dW^[1], db^[1] (캐시에서 z^[1], z^[2], z^[3]를 옮긴다) <br> \*주의사항<br>
   정방향 반복은 입력 데이터 X로 초기화한다<br>
   역방향 반복은 da^[l] (=-y/a+(1-y)/(1-a)로 초기화, 벡터화된 구현이라면 dA^[L]<br>
   <br>
3) 심층 신경망에서의 정방향전파<br>
   첫 번째 층 <br>
   X : z^[1] = W^[1]x + b^[1] (x=a^[0])<br>
   a^[1] = g^[1](z^[1])<br>
   두 번째 층<br>
   z^[2] = W^[2]a^[1] + b^[2]<br>
   a^[2] = g^[2](z^[2])<br>
   네 번째 층<br>
   z^[4] = W^[4]a^[2] + b^[4]<br>
   a^[4] = g^[4](z^[4])<br>
   <br>
   -> 일반적인 정방향 전파의 수식<br>
   z^[l] = W^[l]a^[l-1] + b^[l]<br>
   a^[l] = g^[l](z^[l])<br>
   <br>
   -> 벡터화된 수식<br>
   Z^[1] = W^[1]X + b^[1] (X=A^[0])<br>
   A^[1] = g^[1](Z^[1])<br>
   Z^[2] = W^[2]A^[1] + b^[2]<br>
   A^[2] = g^[2](Z^[2])<br>
   <br>
   Z : Z^[2](1) Z^[2](2) ... Z^[2](m) 이런 식으로 m번째 학습 데이터까지하고 열에 저장<br>
   A도 마찬가지<br>
   <br>
   Z^[l] = W^[l]A^[l-1] + b^[l]<br>
   A^[l] = g^[l](Z^[l]) <br>
   명시적인 반복문 사용할 수 밖에 없다<br>
   <br>
4) 행렬의 차원을 알맞게 만들기<br>
   L=5<br>
   4개의 은닉층과 1개의 출력층<br>
   n^[0]=2, n^[1]=3, n^[2]=5, n^[3]=4, n^[4]=2, n^[5]=1<br>
   <br>
   Z^[1] = W^[1]X + b^[1]<br>
   <br>
   z : 첫 번째 은닉층에 대한 활성화 벡터 <br>
   (n^[1],1) (3,1)<br>
   <br>
   x : 입력 특성<br>
   (n^[0],1) (2,1)<br>
   <br>
   W<br>
   (n^[1],n^[0]) (3,2)<br>
   W^[l]<br>
   (n^[l],n^[l-1]) (3,2)<br>
   <br>
   b^[l]<br>
   (n^[l],1)<br>
   <br>
   dW^[l]의 차원 = W^[l]의 차원<br>
   db^[l]의 차원 = b^[l]의 차원<br>
   <br>
   Z^[1] = W^[1] X + b^[1]<br>
   (n^[1],m) (n^[1],n^[0]) (n^[0],m) (n^[1],1) -> 파이썬 브로드캐스팅으로 (n^[1],m) 된다<br>
   <br>
5) 왜 심층 신경망이 더 많은 특징을 잡아 낼수 있을까요?<br> #직관 1<br>
   <얼굴 사진><br>
   -> 신경망의 첫 번째 층 : 사진을 보고 모서리가 어디에 있는지 파악한다<br>
   -> 픽셀을 그룹화해 모서리를 형성한다<br>
   -> 감지된 모서리와 그룹화된 모서리를 받아서 얼굴의 일부를 형성한다<br>
   (어떤 뉴런에서는 눈, 다른 뉴런에서는 코 찾기)<br>
   -> 서로 다른 얼굴의 일부를 모아 서로 다른 종류의 얼굴 감지한다<br>
   <br>
   신경망의 초기 층 : 모서리와 같은 간단한 함수 감지<br>
   이후의 층 : 복잡한 함수 학습<br>
   간단한 것 찾기 -> 모아서 더 복잡한 것 찾기 -> 더 복잡한 것 찾기 (사람의 뇌처럼)<br>
   <br> \*모서리 탐지기 : 이미지에서 상대적으로 작은 영역을 본다<br>
   얼굴 탐지기 : 이미지의 더 넓은 영역을 본다<br>
   <br>
   <음성 데이터><br>
   낮은 단계의 음성 파형 특징 탐지 (음소 탐지) -> 단어 인식 -> 구나 문장 인식<br>
   <br> #직관 2<br>
   회로 이론 : 로직 게이트의 서로 다른 게이트에서 어떤 종류의 함수를 계산할 수 있을지<br>
   <br>
   상대적으로 은닉층의 개수가 작지만 깊은 심층 신경망에서 계산할 수 있는 함수가 있다. 그러나 얕은 네트워크로 같은 함수를 계산하려고 하면, 즉 충분한 은닉층이 없다면 기하급수적으로 많은 은닉 유닛이 계산에 필요하게 된다<br>
   <예시><br>
   모든 입력 특성에 대한 XOR을 계산한다고 하자<br>
   x1 XOR x2 XOR x3 XOR ... xn<br>
   XOR 트리 그리면 결국 y 출력된다<br>
   네트워크의 깊이 = O(logn)<br>
   노드의 개수 = 네트워크의 게이트 수<br>
   <br>
   파라미터나 하이퍼파라미터를 조절해 신경망의 알맞은 깊이를 찾는다<br>
   <br>
6) 심층 신경망 네트워크 구성하기<br>
   •  l  번째 층에서 정방향 함수는 이전 층의 활성화 값인 a ​[l −1 ]​​ 을 입력으로 받고, 다음 층으로  a ​[l ]​​ 값을 출력으로 나오게 한다. 이때 선형결합된 값인  z ​[l ]​​ 와 변수  W ​[l ]​​,b ​[l ]​​ 값도 캐시로 저장해둔다.<br>
   •  l   번째 층에서 역방향 함수는 d a ​[l ]​​ 을 입력으로 받고, d a ​[l ]​​ 를 출력한다. 이때 업데이트를 위한  d W ​[l ]​​ 와 d b ​[l ]​​ 도 함께 출력한다. 이들을 계산하기 위해서 전방향 함수때 저장해두었던 캐시를 쓰게 된다.<br>
   <br>

7) 변수 vs 하이퍼파라미터<br>
   변수 : 신경망에서 학습 가능한 W와 b<br>
   하이퍼파라미터 종류<br> -학습률(learning rate, \alphaα )<br> -반복횟수(numbers of iteration)<br> -은닉층의 갯수(numbers of hidden layer, L)<br> -은닉유닛의 갯수(numbers of hidden units)<br> -활성화 함수의 선택(choice of activation function)<br> -모멘텀항(momentum term)<br> -미니배치 크기(mini batch size)<br>
   <br>
   매개변수인 하이퍼파라미터를 결정함으로서 최종 모델의 변수를 통제할 수 있다.<br>
   하이퍼파라미터는 결정 된것이 없으며, 여러번의 시도를 통해 적합한 하이퍼파라미터 를 찾아야한다.<br>
