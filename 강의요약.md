# 강의 요약

## 1. 더 많은 층의 심층 신경망

- deep neural network?
    - logisitic regression = shallow network
    - 더 복잡한 문제를 정확하게 예측하는 것이 가능해짐 (hidden layer의 수 역시 hyper parameter로 작용)
    - 표기법
        
        ![Untitled](자료/Untitled.png)
        

---

## 3. 심층 신경망에서의 정방향 전파

- 각 층에 대해 Z, 활성값을 구하기 위한 반복문은 불가피하므로 그냥 사용하자.

---

## 4. 행렬의 차원을 알맞게 만들기

- 행렬의 차원 체크하기
    - hidden layer의 unit 수와 관련
        
        Z와 X의 차원을 파악한 후 W의 차원 고정 ( bias는 차원에 영향X)
        W^[l] = ( n^[l], n^[l-1] ) = dW^[l]
        b^[l] = ( n^[l], 1 ) = db^[l]
        m개의 sample이 있다고 가정할 때 행렬의 차원은 
        Z^[l], A^[l] = ( n^[l], m)이 된다. (역전파에서의 dZ, dA도 마찬가지)
        
- 역전파 구현하는 경우에도 dW^[l]의 차원은 W^[l]의 차원과 같아야한다.

---

## 5. 왜 심층 신경망이 더 많은 특징을 잡아 낼 수 있을까요?

- 네트워크가 깊어질 수록 이미지의 많은 특징을 잡아낼 수 있다. 이를 모아 복잡한 특징도 찾을 수 있다.
- 회로 이론으로부터 온 개념, 네트워크 수가 깊을 수록 정답을 도출하는 함수에 대한 추측이 더 정확해진다.

---

## 6. 심층 신경망 네트워크 구성하기

- 정방향 함수 : a^[l-1]로 a^[l]를 계산, w^[l], b^[l]을 필요로, 이 때 z^[l]을 저장
- 역방향 함수 : da^[l]로 da^[l-1]을 계산, 이 때 저장된 z^[l]을 이용해서 계산
    - w^[l], b^[l]도 저장해서 계산하는 것이 좀 더 편리하다.

---

## 2. 정방향전파와 역방향전파

- *l* 번째 층에서 정방향 전파는 이전 층의 활성화 값인 *a*[−1]을 입력으로 받고, 다음 층으로 ] 값을 출력이 된다. 이때 선형결합된 값인 *z*] 와 변수 *W*],*b*] 값은 역방향 전개를 위해 저장해둔다.
- *l* 번째 층에서 역방향 전파는 *da*[] 을 입력으로 받고, *l-1*] 를 출력한다. 이때 업데이트를 위한 *dW*] 와 *db*] 도 함께 출력
- 정방향 : X → ReLU → ReLU → Sigmoid(이중분류)→cost function 
역방향 : Y → dw[3],db[3] → dw[2],db[2] → dw[1],db[1]
- da^[L] = -y/a + (1-y)/(1-a)

---

## 7. 변수 vs 하이퍼파라미터

- parameters : learning rate, # of iterations, # of hidden layers(L), # of hidden units, choice of activation function…
- hyper parameter값에 따라 cost J의 변화를 파악하고 적합한 수치를 설정

---

## 8. 인간의 뇌와 어떤 연관이 있을까요?