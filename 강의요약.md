# 강의 요약

## ****Softmax Regression****

- 여러 클래스 분류 시 사용하는 Softmax(logistic regression을 일반화한 것)

![Untitled](자료/Untitled.png)

→ 두 개 이상의 클래스가 존재할 때

(C = # of class) 위의 경우엔 4 (0,1,2,3)

![Untitled](자료/Untitled%201.png)

y_hat 직전의 vector는 각각 p(other|x), p(cat|x), p(dog|x), p(bc|x)을 의미 / 이를 softmax 로구현할 수 있다. 

t = e^(z[l])일 때 t(i) = sum(t)를 이용해서 해당하는 값(해당 클래스가 될 확률)을 구할 수 있다. (합이 1인 값을 내놓는다)

→ 임시 변수 t를 이용한 normalize과정을 softmax함수의 일환으로 볼 수 있다. 

→ g에 대한 특이한 점은 벡터를 받아들여서 크기가 같은 벡터를 result로 가져온다.

- Softmax examples
    - 두 class 간의 경계는 선형 결정 경계를 갖는다. (여러 개의 선형 함수를 이용해서 다중 클래스에 대한 구분이 가능함)

## ****Softmax 분류기 훈련시키기****

↔ One-hot coding (hardmax) : 가장 가능성이 높은 class에 대해 확률을 1을 부여하는 방법(소프트맥스와 반대된다)

- softmax regression generalizes logistic regression to C classes. (로지스틱 회귀에 대한 일반화라고 생각하면 된다 / if C=2 → softmax reduces to logistic regression)

---

학습시키기

- 신경망을 학습시키기위한 하나의 훈련 샘플에서의 손실 함수

![Untitled](자료/Untitled%202.png)

→ 실제값인 y를 이용해서 손실함수를 계산 (ex. y=[0,1,0,0] 인 경우 L = -y_2log(y_hat_2)가 된다 

관측 결과가 어떻게 되든 y를 이용해서 그 클래스에 대응하는 확률을 가능한 크게 만드는 것 (통계학의 최대 우도 추정과 비슷)

- 전체 훈련 세트에 대한 비용 함수 J
    
    ![Untitled](자료/Untitled%203.png)
    

---

- softmax 출력층이 있는 경우의 경사 하강법
    
    softmax와 손실함수를 결합한 역전파의 값으로 dz^[L]은 아래와 같이 구할 수 있다
    
    이 때 y_hat, y는 모두 C*m의 크기를 갖는다 ( C = # of class, m = # of training sample)
    

![Untitled](자료/Untitled%204.png)

## 지역 최적값 문제

![Untitled](자료/Untitled%205.png)

![Untitled](자료/Untitled%206.png)

축에 해당하는 parameter를 최적화할 때 지역 최적화 값으로 수렴할 수 있는 위험이 많다. 

그러나 대부분의 경우, 경사가 0일 때의 값은 지역 최적화 값이 아닌 안장점(saddle point)이다.

→ 안장점 ( 각 방향에서 볼록, 오목함수의 형태를 보임) 

---

- 안정 지대에 의한 학습의 지연이 문제가 될 수 있다.
    
    → 안정지대는 아주 오랫동안 미분값이 0으로 유지된다. saddle point에 도달해야 안정 지대를 벗어날 수 있는데 이 지점에 도달하기까지 아주 오랜 시간이 걸린다.
    
    ![Untitled](자료/Untitled%207.png)
    
    이를 해결하기 위해 RMSprop, Adam 등의 알고리즘의 도움을 받을 수 있다.
    
    <aside>
    💡 RMSprop, Adam이 도움되는 이유
    1. 각각 매개변수에 대한 적응적인 학습률을 제공한다. Gradient를 기반으로 매개변수마다 다른 학습 속도를 가질 수 있도록 하여 saddle point를 빠져나오는 데에 도움이 된다.
    2. 모맨텀 개념을 포함한다. 이전 단계의 gardient 정보를 바탕으로 다음 단계의 움직임에 대한 업데이트를 진행하므로 saddle point를 벗어나기 쉽다.
    
    </aside>
    

## ****Deep Learning Frameworks****

[Deep Learning Frameworks (C2W3L10)](https://www.youtube.com/watch?v=AK6r-llqogg)

→ 큰 모델의 구성이 필요해지면서 프레임워크를 사용하지 않은 구현은 점점 비효율적이게 되어간다. 따라서 모델 구현을 위한 프레임워크의 사용법을 익히는 것은 불가피하다.

---

- 딥러닝 프레임워크의 종류(6년 전 영상)
    
    caffe/caffe2
    
    CNTK
    
    DL4J
    
    Keras
    
    Lasagne
    
    mxnet
    
    PaddlePaddle
    
    TensorFlow
    
    Theano
    
    Torch…
    

---

- Choosing deep learning frameworks
    1. Ease of programming
    2. Running speed
    3. Truly open (frame워크가 개방적인 것

## ****Tensorflow****

<aside>
💡 위 강의는 Tensorflow 1(구버전)을 바탕으로 진행되는 것으로 현재는 버전 2를 쓰고 있고, 1과는 api 및 코드의 흐름 등이 상당히 다름을 참고하기

</aside>

[TensorFlow Core](https://www.tensorflow.org/tutorials?hl=ko)

- Tensorflow에서 J에 대해 최소화하는 방법
    
    ```python
    import numpy as np
    import tensorflow as tf
    
    # 고정된 cost function을 사용하지 않고 그 때 그 때 대입하여 사용하도록
    coefficient = np.array([[1.],[-10.],[25.]])
    
    w = tf.Variable(0,dtype=tf.float32) # 매개변수 정의
    x = tf.placeholder(tf.float32,[3,1]) # 훈련 데이터의 정의/ placeholder -> 값을 나중에 넣도록 하는 함수
    # cost = tf.add(tf.add(w**2, tf.multiply(-10,w)),25) # 비용함수 정의, w^2-10w+25
    # define optimizer to find minimize of cost
    cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]
    train = tf.train.GradientDescentOptimizer(0.01)minimize(cost) # learning rate=0.01
    
    init = tf.global_variables_initializer()
    session = tf.Session()
    session.run(init) # 전역변수 초기화
    print(session.run(w)) # 아직 계산 전이므로 w=0
    
    session.run(train, feed_dict]{x:coefficients}) # 경사하강 1번 실행
    print(session.run(w))
    
    for i in range(1000): # 경사하강 1000번 실행
    	session.run(train)
    print(session.run(w))
    
    ```
    
    텐서플로우는 cost함수를 명시해주면 자동으로 미분을 계산하여 최소화할 수 있다.
    
    computation graph를 구성하여 비용함수를 계산하여 정방향을 구현하면 역방향 함수는 이미 구현되어 있으므로 심층 신경망을 학습시킬 때 해당 과정을 생략할 수 있다.
    
    (정방향 함수 구현을 위해 내장함수를 이용하면 된다)