# 강의 요약

## 미니 배치 경사하강법

- 벡터화 : m개의 샘플에 대한 계산 (반복문 없이)
    
    → 훈련 샘플에 대해 벡터화한 후에 벡터 간의 계산을 진행할 수 있으므로
    
    이 때 m이 매우 크다면 그 속도는 여전히 느리다. 단계의 반복을 위해 더 작은 훈련 세트로 나눈다 → mini-batch gradient descent
    
    ```python
    # Mini-batch gradient descent
    for i in range(1,5001):
    	forward prop on X^{t}
    		Z[i] = W[i]X[t]+b[i]
    			A[1]=g[1](Z[1]) ... # L번째 까지
    	compute cost J = 1/1000 sum(L(y,y')) + (정규화항)
    	///
    	backprop to compute gradients 
    # 총 5000번의 epoch을 학습
    ```
    

## ****미니 배치 경사하강법 이해하기****

- batch gradient descent : 모든 반복에서 모든 훈련 세트를 학습
mini-batch gradient descent : 일부 세트만 학습하며 반복 → 항상 하향하지 않음(경향성을 띰)

![Untitled](자료/Untitled.png)

- 미니배치의 크기
    - 최대 훈련세트 수(m)인 경우 : 너무 길고 오래 걸림
    - size1가 1인 경우(SGD)  : 작은 학습률로 노이즈를 줄일 수 있으나, 한 번에 하나의 훈련 세트를 진행하기 때문에 매우 비효율적
    
    → 1과 m사이의 적절한 값을 지정하여 사용, 항상 최솟값으로 수렴하진 않지만 일관되게 global minimum으로 향하는 경향을 보임
    
- 크기 설정하기
    - 훈련 세트가 작으면 그냥 GD를 사용(1 iteration = 1 epoch)
    - 훈련 세트가 크면 일반적으로 64~512를 자주 사용 ( 컴퓨터 구조 상 2의 제곱인 값이 가장 효율적) → 그 중 가장 비용함수가 작은 값을 채택

## ****지수 가중 이동 평균****

- 지수 가중 이동 평균 (Exponentially Weighted Average)
    
    이전 값과 현재 값의 비율을 지정해서 업데이트하는 방식
    
    최근의 데이터에 더 많은 영향을 받는 데이터들의 평균 흐름을 계산하기 위해 지수 가중 이동 평균을 구한다
    
    ![Untitled](자료/Untitled%201.png)
    
    베타가 커질수록 근사된 단조로운 곡선이 됨
    

## ****지수 가중 이동 평균 이해하기****

- Vn을 세타로 표현하기 → 지수적으로 감수하는 함수꼴이 됨

![Untitled](자료/Untitled%202.png)

## ****지수 가중 이동 평균의 편향보정****

- 편향 보정을 통해 더 정확한 평균을 계산할 수 있다.
- Vt를 (1-베타^t)로 나누어주면서 초기 값을 실제값과 비슷하게 하나 머신러닝에서는 시간이 지나면서 분모가 1에 가까워지면서 일치하게 됨. 초기값이 중요할 경우에만 구현한다.

## ****Momentum 최적화 알고리즘****

![Untitled](자료/Untitled%203.png)

- 진동하면서 수렴 속도가 느리고, overshutting에 의한 발산도 고려하여 학습률을 적절히 조정해야한다.
- Momentum을 이용한 gradient descent에서는 mini batch에 대한 dw, db를 이용해서 조절한다
    
    ![Untitled](자료/Untitled%204.png)
    
    위와 같은 알고리즘을 거치며 w를 업데이트 한다.
    
    이를 이용한 업데이트는 매 단계의 경사 하강 정도를 완만하게 만든다.
    
- 구현 방법

![Untitled](자료/Untitled%205.png)

## ****RMSProp(root mean squre) 최적화 알고리즘****

- Sdw를 이용
    
    ![Untitled](자료/Untitled%206.png)
    
    도함수의 제곱을 가중평균 → 클수록 업데이트 시 더 큰 값으로 나눠주기 때문에 기존 학습률보다 더 작은 값으로 업데이트 된다. 반대로 미분값이 작은 곳에서는 업데이트 시 작은 값으로 나눠주기 때문에 기존 학습률보다 큰 값으로 업데이트 된다. 더 빠르게 수렴하는 효과를 보이며 그냥 Momentum 최적화 알고리즘보다 업데이트 시의 진동을 줄이는 데 도움이 된다. 
    

## ****Adam(Adaptve moment estimation) 최적화 알고리즘****

- Momentum과 RMSProp을 섞은 알고리즘으로 아래와 같은 알고리즘을 갖는다

![Untitled](자료/Untitled%207.png)

- momentum에 대한 베타1, RMS에 대한 베타2 각각을 고려
    - 각각 0.9, 0.99를 추천한다.

## ****학습률 감쇠 (learning rate decay)****

- 시간에 따라 학습률을 점점 줄이는 것
    - 학습 초기에 훨씬 큰 스텝으로 진행하고, 학습이 수렴할수록 학습률이 느려저 작은 스텝으로 진행

<aside>
💡 GPT

1. **Step Decay:**
    - The learning rate **is reduced by a constant factor** after a fixed number of training epochs or steps.
    - Formula: lr=initial_lr×decay_factorfloor(step_sizeepoch)
        
        lr=initial_lr×decay_factorfloor(epochstep_size)
        
2. **Exponential Decay:**
    - The learning rate **is reduced exponentially over time.**
    - Formula: lr=initial_lr×exp(−decay_rate×epoch)
        
        lr=initial_lr×exp(−decay_rate×epoch)
        
3. **Inverse Decay:**
    - The learning rate **is decreased proportionally to the inverse of the epoch number.**
    - epoch이 커질 수록 작아짐 → 업데이트 조금씩 진행
    - Formula: lr=1+decay_rate×epochinitial_lr
        
        lr=initial_lr1+decay_rate×epoch
        
4. **Polynomial Decay:**
    - The learning rate **is reduced according to a polynomial function of the form** lr=initial_lr×(1−max_epochsepoch)power
        
        lr=initial_lr×(1−epochmax_epochs)power
        
5. ****Cosine Annealing Decay:(가장 많이 사용)**
    - The **learning rate follows a cosine function**, decreasing in a smooth curve.
    - Formula
    
    ![Untitled](자료/Untitled%208.png)
    
6. **Warmup and Decay:**
    - A combination of a warm-up phase where the learning rate **starts low and gradually increases, followed by a decay phase where it decreases**.
    - This helps the model converge quickly at the beginning and then fine-tune with a lower learning rate.
7. **Adaptive Methods:**
    - Adaptive learning rate methods, such as **Adam or Adagrad**, adapt the learning rate for **each parameter individually based on their historical gradients**. While these methods do not have a fixed decay schedule, they implicitly adjust the learning rates during training.
</aside>