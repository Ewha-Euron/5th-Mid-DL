# 강의요약

## 3. 파이썬과 벡터화

### (1). 벡터화

- for문을 없애기 위한 vectorization
- vectorization?
    - logistic regression에서 z를 계산하기 위해 i가 1부터 n_x까지 반복하며 계산
    - 시간 절약을 위해 각 경우를 vector로 만들어서 numpy library를 이용해서 계산
    
    계산량이 같을 때 for문이 훨씬 더 시간이 걸리는 것을 확인할 수 있다.
    
    ![Untitled](자료/Untitled.png)
    
    → 될 수 있으면 for문을 사용하지 않는다
    

---

### (2) 더 많은 벡터화 예제

- 가능한 for문의 사용을 줄여 계산의 시간 복잡도를 줄인다.
- u=Av → u=np.dot(A,v)로 정의해서 계산 진행
    
    ex) vector V에 대해서 e^v1,v2…를 계산하고 싶을 때
    
    ```python
    # for 문의 사용
    u=np.zeros((n,1))
    for i in range(n):
    	u[i] = math.exp(v[i])
    
    # for문 사용 x
    import numpy as np
    u=np.exp(v)
    ```
    
- numpy의 다양한 함수
    - np.log(v)
    - np.abs(v)
    - np.maximun(v,0)
    - v**2 (모든 원소에 제곱)
    - 1/v (역수를 반환)
- apply to logistic regression
    
    ![Untitled](자료/Untitled%201.png)
    
    → 두번째 for문을 없애기. dw_1,dw_2를 초기화하는 대신에 dw vector를 지정
         `dw=np.zeros((n_x,1))`
    
    `dw += x(i)dz(i)`
    
    `dw /= m`
    
    로 바꿔서 for문을 제거할 수 있다.
    

### (3) 로지스틱 회귀의 벡터화 (정방향)

- for문을 사용하지 않는 신경망
    - 정방향 전파(활성값을 한 줄의 코드로 간단히 구현할 수 있다)
        
        m개의 sample이 있을 때 (w,b를 이용한 z값 → sigmoid를 이용한 pred_y 구하기)
        
        ![Untitled](자료/Untitled%202.png)
        
        →를 m번 반복
        
    - X = [x(1),x(2),x(3)…]의 train set을 모은 행렬
    Z = wX+b로 행렬 X를 이용해서 m개의 sample에 대한 z를 한 번에 구한다
    (이 때 Z의 크기는 (1,m))
    `Z=np.dot(w.T,X)+b`

### (4) 로지스틱 회귀의 경사 계산을 벡터화 하기

- m개의 훈련 샘플 수에 대한 for문 제거하기
    - dZ = [dz^(1), dz^(2), dz^(3),…]로 (1,m)크기의 행렬
    dZ= A-Y (이 때 A는 활성값을 모아둔 행렬, Y는 각 sample에 대한 label을 의미)
    - `db=1/m(np.sum(dZ))`
    dw=1/m(X,tran(dZ))
- apply logistic regression
    
    ![Untitled](자료/Untitled%201.png)
    
    → 첫번째 for문 (i는 1~m까지)를 제거
    
    ```python
    Z=trans(w)X + b = np.dot(w.T, X) + b
    A = sigmoid(Z)
    dZ = A - Y
    dw = 1/m(X,trans(dZ)
    w:= w-adw
    b:= b-adb
    # a는 elarning rate를 의미
    ```
    

### (4) 파이썬의 브로드캐스팅

![Untitled](자료/Untitled%203.png)

- 서로 다른 크기의 행렬을 연산이 가능하도록 조정
    
    ex) (m,n)과 (m,1)행렬 간의 연산을 진행할 때 자동으로 n개의 (m,1)을 이용해서 연산한다
    

### (5) 파이썬과 넘파이 벡터

- broadcasting을 사용할 때 유의할 점

![Untitled](자료/Untitled%204.png)

### (6) Jupyter/iPython Notebooks 가이드

[[MAC]쥬피터노트북설치하기.pdf](자료/JupyterNotebook설치.pdf)

### (7) 로지스틱 회귀의 비용함수 설명

- pred_y는 y가 1일 확률을 얘기하는 것과 비슷하다

![Untitled](자료/Untitled%205.png)

위의 두 개의 수식을 하나의 수식으로 합치면 아래와 같다

![Untitled](자료/Untitled%206.png)

이 때 y가 1일 경우 P(y|x) = pred_y, y가 0일 경우엔 1-pred_y가 된다.

위 식을 log함수에 대해 정리하면 아래와 같다. 하나의 훈련 샘플에 대한 식으로 아래 식이 최대가 되는 pred_y를 찾는다. (-logP(y|x)가 최소화하는 것을 목표로 한다.)

![Untitled](자료/Untitled%207.png)

위 식을 이용해서 최종적으로 비용함수를 정의하면 아래와 같은 식을 갖는다.

![Untitled](자료/Untitled%208.png)