자연어 처리의 모든 것<br>

1. 자연어 처리 활용 분야와 트렌드<br><br>

1) Natural language processing (자연어 처리)<br><br>

주요 학회 : ACL, EMNLP, NAACL<br>
학문 분야<br>
Low-level parsing : Tokenization, stemming - 어미<br>
Word and phrase level : NER(Named Entity Recognation) - 고유명사 인식, POS(Part-Of-Speech) tagging – 품사, 성분 무엇인지<br>
Sentence level : 감성 분류(Sentiment Analysis) - 긍정, 부정, 기계 번역(Machine Translation) - 적절한 한글로 번역<br>
Multi-sentence and paragraph level : 논리적 내포 및 모순관계 예측(Entailment Prediction) - 어제 존이 결혼했다가 참일 경우 어제 최소 한명이 결혼했다가 무조건 참, 독해기반 질의응답(question answering) - 질문을 정확하게 이해하고 답에 해당하는 정보 바로 보여줌, 챗봇(dialog systems), 요약(summarization)<br><br>

2. Text mining (텍스트 마이닝) - 빅데이터 분석과 관련, 키워드의 빈도수를 분석해 소비자 반응 얻어냄<br><br>

주요 학회 : KDD, The WebConf(前 WWW), WSDM, CIKM, ICWSM<br>
학문 분야<br>
Extract useful information and insights from text and document data<br>
문서 군집화(Document clustering) - 서로 다른 키워드지만 비슷한 의미들을 그루핑해서 분석할 필요생김 ex) 토픽 모델링<br>
Highly related to computational social science : 통계적으로 사회과학적 인사이트 산출, 트위터, 페이스북,...<br><br>

3. Information retrieval (정보 검색) - 구글, 네이버 등의 검색 기술 연구, 검색기술이 성숙해있기 때문에 상대적으로 발전이 느림<br><br>

주요 학회 : SIGIR, WSDM, CIKM, Recsys<br>
학문 분야<br>
Highly related to computational social science<br>
정보 검색 분야, 추천 시스템 – 비슷한 노래들 자동 추천, 알고리즘 자동 추천<br><br>

<자연어 처리 분야의 발전 과정><br>
느리지만 꾸준히 발전해왔다<br>
주어진 텍스트 데이터 단어 단위로 분리 -> 벡터로 변경 (워드 임베딩)<br>
워드 벡터가 시퀀스로 주어지기 때문에 RNN 모델이 적합하다<br>
RNN 모델로는 LSTM, GRU가 사용해오다가 2017년 구글에서 나온 Transformer모델 사용하게 되었다<br>
현재 대부분의 자연어처리 모델들은 Transformer모델을 사용하고 있다. Transformer 모델은 주로 사용되던 '기계 번역' 분야를 넘어 현재는 영상/신약개발/시계열 예측 등에서도 다양하게 사용되고 있다. 최근에는 자가지도 학습(self-supervised Learning)이 가능한 BERT, GPT 와 같은 모델의 유행하고 있다. 입력 중 일부를 가리고 앞뒤 문맥을 보고 단어를 맞추게 하는 것<br><br>

2. 기존의 자연어 처리 기법<br>
   <Bag-Of-Words (단어 가방 모형)><br>
   : 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법<br><br>

John really really loves this movie. Jane really likes this song<br><br>

1. constructing the vocabulary containing unique words<br>

- vocabulary에 8개 단어를 등록한다. 중복된 단어는 한번만 등록한다<br><br>

2. encoding unique words to one-hot vectors<br>

- categorical variable -> one-hot vector<br>
- john: [1 0 0 0 0 0 0 0]<br>
- 어떤 단어쌍이든지 거리가 루트2<br>
- 어떤 단어쌍이든지 유사도가 0 (모두가 동일한 관계를 가진다)<br>
- 단어별로 가방을 준비하고 나타나면 가방에 넣어 그 수를 세어 벡터로 나타낸다<br><br>

<Naive Bayes Classifier for Document Classification><br>
: 머신러닝의 주요 알고리즘으로 분류에 있어 준수한 성능을 보여주고 있다<br>
[Bayes' theorem]<br>
: 조건부 확률을 계산하는 방법<br><br>

다른 단어들이 분류하고자 하는 문장에 많이 등장했을지라도, Training data 에서 1번이라도 등장하지 않았다면 모든 단어들의 확률 곱으로 인해 0이 된다. -> regularization으로 해결한다<br><br>

3. Word Embedding - (1)Word2Vec<br>
   <Word Embedding><br>
   : 각 단어를 좌표공간에 최적의 벡터로 표현하는(임베딩하는) 기법<br>
   유사한 단어는 가까이, 유사하지 않은 단어는 멀리 위치하는 것을 '최적의 좌표값'으로 표현할 수 있다.<br><br>

<Word2Vec><br>
문장 내에서 비슷한 위치에 등장하는 단어는 유사한 의미를 가질 것이다<br>
Tokenizing -> Vocabulary -> 중심단어를 위주로 학습 데이터를 구축. 중심단어가 study 라고 한다면, (I study), (study I) , (study math)<br><br>

<Word2Vec의 계산><br>
문장의 단어의 갯수만큼에 Input, Output 벡터 사이즈를 입력/출력해준다. 이 때 연산에 사용되는 히든 레이어(hidden layer,은닉 층)의 차원(dim)은 사용자가 파라미터로 지정할 수 있다.<br>
실제로 Tensorflow나 Pytorch와 같은 프레임워크에서는 임베딩 레이어와의 연산은 0이 아닌 1인 부분, 예를 들어 [0,0,1]의 벡터인 경우는 3번째 원소와 곱해지는 부분의 컬럼(column)만 뽑아서 계산해준다. <br>
마지막 결과값으로 나온 벡터는 softmax 연산을 통해 가장 큰 값이 1, 나머지는 0으로 출력된다.<br><br>

<Word2Vec의 특성><br>
워드투벡터를 통해 단어를 임베딩하면 queen - king 그리고 woman - man , 마지막으로 aunt - uncle 의 벡터가 비슷한 것을 볼 수 있다. 해당 결과가 의미하는 것은 여성과 남성의 관계성을 잘 학습했다는 것을 의미한다.<br><br>

<Word intrusion detection><br>
각 단어별로 나머지 단어들과 유클리디안 거리를 계산한 후 평균값을 낸다<br>

<Application of Word2Vec><br>
Word2Vec은 그 자체로도 의미가 있지만, 뿐만 아니라 다양한 테스크에서 사용되고 있다. <br>
Machine translation : 단어 유사도를 학습하여 번역 성능을 더 높여준다.<br>
Sentiment analysis : 감정분석, 긍부정분류를 돕는다.<br>
Image Captioning : 이미지의 특성을 추출해 문장으로 표현하는 테스크를 돕는다. <br><br>

4. Word Embedding - (2)GloVe<br>
   <Glove : Global Vectors for Word Representation><br>
   : 사전에 미리 각 단어들의 동시 등장 빈도수를 계산하며, 단어간의 내적값과 사전에 계산된 값의 차이를 줄여가는 형태로 학습한다. <br>
   Word2Vec는 특정한 입출력쌍이 자주 등장한 경우 여러번에 걸쳐 학습되어 내적값이 커진다. 하지만 Glove는 단어쌍이 동시에 등장한 횟수를 미리 계산하고 로그값을 취한 것을 Ground Truth로 사용해 반복계산을 줄일 수 있다. 따라서 Word2Vec보다 더 빠르게 동작하며, 더 적은 데이터에서도 잘 동작한다. <br><br>

<사전 학습된 Glove 모델><br>
사전에 이미 대규모 데이터로 학습된 모델이 오픈소스로 공개되어 있다. 해당 모델은 위키피디아 데이터를 기반으로 하여 6B token만큼 학습 되었으며, 중복 제거 시에도 단어의 개수가 무려 40만개(400k)에 달한다.<br>
학습된 모델을 나타낼 때 뒤에 붙는 "uncased"는 대문자 소문자를 구분하지 않는다는 의미이며, 반대로 "cased"는 대소문자를 구분한다는 의미이다. 예를 들어 Cat과 cat이 uncased에서는 같은 토큰으로 취급되지만, cased에서는 다른 토큰으로 취급된다.
