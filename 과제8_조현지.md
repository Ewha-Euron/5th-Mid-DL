2. 신경망 네트워크의 정규화<br>

1) 정규화<br>
   문제 : 높은 분산으로 신경망이 데이터를 과대적합<br>
   해결1 : 정규화<br>
   해결2 : 더 많은 훈련 데이터 얻는 것 -> 비용이 많이 든다<br>
   <br>
   비용함수 J(w,b)에 정규화 매개변수라고 부르는 λ를 추가한다<br>
   λ : 정규화 매개변수<br>
   -> 개발 세트 혹은 교차 검증 세트를 주로 사용한다<br>
   -> 과대적합을 막을 수 있는 최적의 값을 찾아야 한다!<br>
   <br>
   L1 정규화 - λ/m*(w 절댓값의 합)<br>
   w는 희소해진다, w 벡터 안에 0이 많아진다 -> 모델을 압축하는데 도움이 된다(메모리가 적게 필요하기 때문)<br>
   <br>
   L2 정규화 - λ/2m*(w제곱의 norm), 매개변수 벡터 w의 유클리드 norm의 제곱<br>
   w제곱의 norm : j의 1부터 nx까지 wj^2의 값을 더한 것,<br>
   w의 전치행렬 * w<br>
   L2 norm = Frobenius norm<br>
   *왜 매개변수 w만 정규화할까? 왜 b에 관한 것은 추가하지 않을까?<br>
   -> w : 꽤 높은 차원의 매개변수 벡터. 높은 분산을 가질 때 많은 매개변수를 갖는다<br>
   b : 하나의 숫자<br>
   따라서 거의 모든 매개변수는 b가 아닌 w에 있다<br>
   <br>
   L1 정규화보다 L2 정규화를 훨씬 많이 사용한다<br>
   <br>
   비용함수 : (훈련 샘플의 m까지의 손실의 합/m )+ (λ/2m*(w제곱의 norm))<br>
   <br>
   *경사하강법 계산<br>
   dw^[l] = (from backpropagation)＋(λ/m)*(w^[l])<br>
   dJ/dw^[l] = dw^[l]<br>
   w^[l] = w^[l] - αdw^[l]<br>
   <br>
   *L2 정규화가 weight decay 라고 불리는 이유<br>
   -> weight 에 1보다 작은 값인 ( 1−​αλ/m ) 가 곱해지기 때문<br>
   <br>
   <br>
2) 왜 정규화는 과대적합을 줄일 수 있을까요?<br>
   왜 정규화가 과대적합 문제를 해결하고 분산을 줄이는데 도움이 될까?<br>
   <br>
   비용함수 J = 1부터 m까지 손실의 합 + (λ/2m*(w제곱의 norm))<br>
   L2 혹은 Frobenius norm을 줄이는 것이 왜 과대적합을 줄일 수 있을까?<br>
   #1<br>
   λ를 크게 만들어서 가중치 행렬 w를 0에 상당히 가깝게 설정할 수 있다<br>
   많은 hidden unit을 0에 가까운 값으로 설정해서 hidden unit의 영향력을 줄인다<br>
   -> 훨씬 더 간단하고 작은 신경망이 된다<br>
   -> 로지스틱 회귀 유닛에 가까워진다<br>
   -> high bias의 경우와 가깝게 만든다<br>
   -> λ값을 잘 조정하기!<br>
   <br>
   *λ값을 아주 크게 -> w는 0에 가까워짐<br>
   간단한 네트워크는 과대적합 문제가 덜 일어난다<br>
   <br>
   #2<br>
   tanh 활성화 함수를 사용한다고 가정<br>
   g(z) = tanh(z)<br>
   -z가 아주 작은 경우 -> tanh의 선형 영역을 사용<br>
   -z가 더 작아지거나 커지는 경우 -> 선형을 벗어남<br>
   <br>
   λ가 커질 때 w 작아진다 (비용함수가 커지지 않으려면)<br>
   w작을 때 z가 상대적으로 작은 값 갖게 되면 g(z)는 거의 1차원 함수 -> 모든 층은 선형 회귀처럼 직선의 함수 갖게 된다 -> 모든 층이 선형이면 전체 네트워크도 선형이다<br>
   따라서 선형 활성화 함수를 가진 깊은 네트워크의 경우에도 선형 함수만을 계산할 수 있게 된다<br>
   ->과대적합된 데이터 세트까지 맞추기 어렵다<br>
   <br>
   <정리><br>
   정규화 매개변수가 크면 매개변수 w는 매우 작다<br>
   b의 효과를 무시하면 z의 값은 상대적으로 작다<br>
   전체 신경망은 선형 함수로부터 멀지 않은 곳에서 계산된다 -> 간단한 함수 -> 과대적합의 가능성이 줄어든다<br>
   <br>
   <br>
3) 드롭아웃 정규화<br>
   정규화 방법2 - 드롭아웃<br>
   드롭아웃 : 신경망의 각각의 층에 대해 노드를 삭제하는 확률을 설정하는 것<br>
   삭제할 노드를 랜덤으로 선정 후 삭제된 노드의 들어가는 링크와 나가는 링크를 모두 삭제한다 -> 더 작고 간소화된 네트워크<br>
   <br>
   역드롭아웃 : 노드를 삭제후에 얻은 활성화 값에 keep.prop(삭제하지 않을 확률)을 나눠 주는 것<br>
   -> 기존에 삭제하지 않았을 때 활성화 값의 기대값으로 맞춰주기 위함<br>
   <br>
   <br>
4) 드롭아웃의 이해<br>

- 랜덤으로 노드를 삭제 시키기 때문에, 하나의 특성에 의존 하지 못하게 만듦으로서 가중치를 다른 곳으로 분산 시키는 효과가 있다<br>
- keep.prop 확률은 층마다 다르게 설정 할 수 있다<br>
- 모든 반복에서 잘 정의된 비용함수가 하강하는지 확인하는게 어려워진다. 따라서 우선 드롭아웃을 사용하지 않고, 비용함수가 단조감소인지 확인 후에 사용해야 한다<br>
  <br>
  <br>

5. 다른 정규화 방법들<br>
   #1 데이터 증식<br> -이미지 -> 더 많은 훈련 데이터를 사용함으로서 과대적합을 해결<br>
   대칭, 확대, 왜곡 혹은 회전 시켜서 새로운 훈련 데이터 만든다<br>
   더 많은 정보를 추가해주지는 않지만, 컴퓨터적인 비용이 들지 않고 할 수 있다<br>
   <br>
   #2 조기종료<br>
   : 신경망이 개발 세트의 오차 저점 부근, 즉 가장 잘 작동하는 점일 때 훈련을 멈추는 것
   훈련세트의 오차 : 단조하강함수<br>
   조기종료에서는 개발 세트의 오차도 그려준다<br>
   개발세트의 오차가 어느 순간 부터 하락 하지 않고 증가하기 시작하는 것이라면 과대적화가 되는 시점이다<br>
   단점: 훈련시 훈련 목적인 비용 함수를 최적화 시키는 작업과 과대적합하지 않게 만드는 작업이 있다. 두 작업은 별개의 일이라서 두 개의 다른 방법으로 접근해야 한다. 그러나 조기 종료 두 가지를 섞어 버린다. 따라서 최적의 조건을 찾지 못할 수도 있다<br>
