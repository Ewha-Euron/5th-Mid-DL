1. Softmax Regression<br>
이진 분류 – 0과 1 두가지 선택<br>
<br>
C = 클래스 수<br>
첫 번째 단위 – <br>
출력값 = 입력값 X가 주어졌을 때 기타 클래스가 나올 확률<br>
출력값 y_hat = (4,1)차원의 벡터<br>
y_hat의 합은 1이 된다<br>
<br>
이런 신경망을 얻기 위한 가장 표준적인 모델은 소프트맥스층을 사용하는 것이다<br>
<br>
평소 활성함수 : z^[L] = w^[L]*a^[L-1] + b<br>
소프트맥스 층의 활성함수 : <br>
t = e^z^[L]<br>
a^[L] = e^z^[L] / ti , a_i^[L] = ti / ti<br>
z^[L] = [5, 2, -1, 3]<br>
t = [e^5, e^2, e^-1, e^3] = [148.4, 7.4, 0.4, 20.1], ti = 176.3<br>
a^[L] = t / 176.3<br>
첫 번째 노드의 값 = e^5 / 176.3 = 0.842, 즉, 클래스 0이 될 확률이 84.2%<br>
다음 노드의 값 = e^2 / 176.3 = 0.042<br>
다음 노드의 값 = e^-1 / 176.3 = 0.02<br>
다음 노드의 값 = e^3 / 176.3 = 0.114, 즉, 클래스 3(병아리)이 될 확률이 11.4% <br>
<br>
이 신경망의 출력값 y_hat과도 같은 a^[L]은 (4,1)벡터가 된다. 그 안에는 계산한 이 숫자들이 들어가있다. <br>
알고리즘은 z^[L]이라는 벡터를 취해서 합이 1이 되는 4개의 확률 값을 내놓는다<br>
<br>
<z^[L]에서 a^[L] 되는 과정 요약><br>
e를 취해서 임시 변수 t를 얻어서 정규화한 이 과정을 소프트맥스 활성화 함수로 요약할 수 있다<br>
즉 a^[L]은 z^[L] 벡터에 활성함수 g^[L]을 적용한 것이다<br>
활성화 함수 g의 특이한 점은 (4,1)벡터를 받아서 (4,1)벡터를 내놓는다는 것이다<br>
(이전 함수들은 실수를 받아서 실수를 내놨다)<br>
<br>
<소프트맥스 분류로 할 수 있는 것><br>
학습 세트를 가져와서 비용 함수와 3개의 선택지에 따라 분류하는 소프트맥수 함수를 학습시킨다. <br>
은닉 유닛이 여러 개인, 더 깊은 신경망을 다룬다면 여러 클래스를 분류하기 위해 더 복잡하고 비선형의 경계도 볼 수 있다<br>
2. Softmax 분류기 훈련시키기<br>
z^[L] = [5, 2, -1, 3]  <br>
4개의 클래스를 다루니 z^[L]은 (4,1) 벡터이다. <br>
z^[L]의 가장 큰 원소가 5였고 가장 큰 확률도 첫 번째 확률이다. <br>
소프트맥스 <-> 하드맥스<br><br>

하드맥스는 z벡터를 받아와서 [1, 0, 0, 0] 벡터로 대응시킨다. <br>
소프트맥스는 z벡터를 받아와서 [0.842, 0.042, 0.002, 0.114] 벡터로 대응시킨다.<br>
두 클래스만 다루는 로지스틱 회귀를 일반화했다<br>
소프트맥스에서 C가 2라면 결국 로지스틱 회귀와 같아진다<br>
(출력층 a^[L]은 C=2에서 출력값 두 개를 모아둔 것이다)<br>
(확률 합 1이니까 하나만 계산하면 된다 – 로지스틱 회귀가 하나의 출력값을 계산하는 방식과 같다)<br>

<소프트맥스 출력층을 이용해 신경망을 학습하는 법><br>
y = [0, 1, 0, 0] -> cat을 의미<br>
-> y_1 = y_3 = y_4 = 0 / y_2만 유일하게 1<br>
y_hat = [0.3, 0.2, 0.1, 0.4] -> 신경망이 잘 작동하지 않음<br>
L(y_hat, y) = - y_j*log(y_hat_j) = -y_2*log(y_hat_2) = -log(y_hat_2)<br>
y_j가 0이면 합을 고려할 필요 없다<br>
결국 손실 함수의 값을 작게 만들려고 한다 -> y_hat을 크게 만들려고 한다<br>
입력값 x가 고양이의 사진이었으니 그에 대응하는 출력값인 확률을 최대한 키워야 한다<br>
일반적으로 손실 함수는 훈련 세트에서 관측에 따른 클래스가 뭐든 간에 가능한 한 크게 만드는 것이다.<br>
전체 훈련 세트에 대해 비용 함수 J는 전체 훈련 세트에서 학습 알고리즘의 예측에 대한 손실 함수를 합하는 것이다. <br>
이 비용 함수를 최소로 하기 위해 경사하강법을 써야 한다<br><br>

Y = [y^(1), y^(2),,,y^(m)]<br>
[0,1,0,0]<br>
[0,0,1,0]<br>
[1,0,0,0]<br>
,<br>
,<br>
Y_hat도 똑같이<br>
[0.3,0.2,0.1,0.4]<br>
,<br>
,<br>
(4,m)차원이 된다<br><br>

<softmax 출력층이 있는 경우 경사하강법을 어떻게 구현하는가><br>
마지막 층에서 z^[L]의 미분이 (4,1)벡터인 y_hat에서 (4,1) 벡터인 y를 뺀 것과 같다<br>
클래스가 4개일 때 모두 (4,1)벡터가 된다<br>
dz^[L] : 비용 함수를 z^[L]에 대해 편미분한 것<br>




